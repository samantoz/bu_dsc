{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitdd2052ddce364b29b70b9ef2f4771a5e",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2. Neural Network Classifier with Keras\n",
    "Using the multi-label classifier dataset from earlier exercises (categorized-comments.jsonl in the reddit folder), fit a neural network classifier using Keras. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a multiclass Classifier\n",
    "# Using Keras to construct a feedforward neural network with an output layer with soft-max activation functions\n",
    "\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "#import json library to read data in jsonl file\n",
    "import json\n",
    "#import pandas library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      cat                                                txt\n",
       "0  sports  Barely better than Gabbert? He was significant...\n",
       "1  sports  Fuck the ducks and the Angels! But welcome to ...\n",
       "2  sports  Should have drafted more WRs.\\n\\n- Matt Millen...\n",
       "3  sports            [Done](https://i.imgur.com/2YZ90pm.jpg)\n",
       "4  sports                                      No!! NOO!!!!!"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat</th>\n      <th>txt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sports</td>\n      <td>Barely better than Gabbert? He was significant...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sports</td>\n      <td>Fuck the ducks and the Angels! But welcome to ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sports</td>\n      <td>Should have drafted more WRs.\\n\\n- Matt Millen...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sports</td>\n      <td>[Done](https://i.imgur.com/2YZ90pm.jpg)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sports</td>\n      <td>No!! NOO!!!!!</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "#read in the data as a dataframe\n",
    "filename = \"/home/arindam/Documents/mygithub/bu_dsc/data/raw/categorized-comments.jsonl\"\n",
    "with open(filename, 'r') as f:\n",
    "    jsonl_list = list(f)\n",
    "\n",
    "list1 = []\n",
    "for obj in jsonl_list:\n",
    "    res = json.loads(obj)\n",
    "    list1.append(res)\n",
    "    \n",
    "comments = pd.DataFrame(list1)\n",
    "\n",
    "#display the first few rows of data\n",
    "comments.head()\n",
    "# len(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert text to lowercase and romove punctuation\n",
    "#define a function to clean the text\n",
    "# import the required libraries here\n",
    "#import regular expressions library\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Remove punctuations and special characters, makes lower case\n",
    "    Args: text\n",
    "    Output: text\n",
    "    \"\"\"\n",
    "    text=text.lower() #makes text lowercase\n",
    "    text=re.sub('\\\\d|\\\\W+|_',' ',text) #removes extra white space\n",
    "    text=re.sub('[^a-zA-Z]',\" \", text) #removes any non-alphabetic characters\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                          cat  \\\n",
       "0                     sports   \n",
       "1                     sports   \n",
       "2                     sports   \n",
       "3                     sports   \n",
       "4                     sports   \n",
       "...                      ...   \n",
       "9995  science_and_technology   \n",
       "9996  science_and_technology   \n",
       "9997  science_and_technology   \n",
       "9998  science_and_technology   \n",
       "9999  science_and_technology   \n",
       "\n",
       "                                                    txt  \\\n",
       "0     Barely better than Gabbert? He was significant...   \n",
       "1     Fuck the ducks and the Angels! But welcome to ...   \n",
       "2     Should have drafted more WRs.\\n\\n- Matt Millen...   \n",
       "3               [Done](https://i.imgur.com/2YZ90pm.jpg)   \n",
       "4                                         No!! NOO!!!!!   \n",
       "...                                                 ...   \n",
       "9995  Duuude, you're still on that ancient galaxy s3...   \n",
       "9996  &gt; Moreover, if you buy an unlocked iPhone y...   \n",
       "9997  Why the hell would I want Bing integrated into...   \n",
       "9998  And it overheats like a motherfucker, mostly t...   \n",
       "9999  The same arguments apply to Windows but we sti...   \n",
       "\n",
       "                                                cleaned  ncat  \n",
       "0     barely better than gabbert he was significantl...     1  \n",
       "1     fuck the ducks and the angels but welcome to a...     1  \n",
       "2     should have drafted more wrs matt millen probably     1  \n",
       "3                   done https i imgur com  yz  pm jpg      1  \n",
       "4                                               no noo      1  \n",
       "...                                                 ...   ...  \n",
       "9995  duuude you re still on that ancient galaxy s  ...     2  \n",
       "9996   gt moreover if you buy an unlocked iphone you...     2  \n",
       "9997  why the hell would i want bing integrated into...     2  \n",
       "9998  and it overheats like a motherfucker mostly to...     2  \n",
       "9999  the same arguments apply to windows but we sti...     2  \n",
       "\n",
       "[10000 rows x 4 columns]>"
      ]
     },
     "metadata": {},
     "execution_count": 160
    }
   ],
   "source": [
    "# Cleaning texts in the comments\n",
    "# Using the transformed column for the model\n",
    "# Testing the functions\n",
    "\n",
    "sample_size = 10000\n",
    "sample_cmnts = comments[:sample_size]\n",
    "sample_cmnts['cleaned']=sample_cmnts['txt'].apply(clean_text)\n",
    "# creating a dictionary to replace the string values to numeric\n",
    "d = {'sports':1,'science_and_technology':2,'video_games':3}\n",
    "sample_cmnts['ncat'] = sample_cmnts['cat'].map(d)\n",
    "sample_cmnts.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                         cat  \\\n",
       "0                     sports   \n",
       "1                     sports   \n",
       "2                     sports   \n",
       "3                     sports   \n",
       "4                     sports   \n",
       "...                      ...   \n",
       "9995  science_and_technology   \n",
       "9996  science_and_technology   \n",
       "9997  science_and_technology   \n",
       "9998  science_and_technology   \n",
       "9999  science_and_technology   \n",
       "\n",
       "                                                    txt  \\\n",
       "0     Barely better than Gabbert? He was significant...   \n",
       "1     Fuck the ducks and the Angels! But welcome to ...   \n",
       "2     Should have drafted more WRs.\\n\\n- Matt Millen...   \n",
       "3               [Done](https://i.imgur.com/2YZ90pm.jpg)   \n",
       "4                                         No!! NOO!!!!!   \n",
       "...                                                 ...   \n",
       "9995  Duuude, you're still on that ancient galaxy s3...   \n",
       "9996  &gt; Moreover, if you buy an unlocked iPhone y...   \n",
       "9997  Why the hell would I want Bing integrated into...   \n",
       "9998  And it overheats like a motherfucker, mostly t...   \n",
       "9999  The same arguments apply to Windows but we sti...   \n",
       "\n",
       "                                                cleaned  ncat  \n",
       "0     barely better than gabbert he was significantl...     1  \n",
       "1     fuck the ducks and the angels but welcome to a...     1  \n",
       "2     should have drafted more wrs matt millen probably     1  \n",
       "3                   done https i imgur com  yz  pm jpg      1  \n",
       "4                                               no noo      1  \n",
       "...                                                 ...   ...  \n",
       "9995  duuude you re still on that ancient galaxy s  ...     2  \n",
       "9996   gt moreover if you buy an unlocked iphone you...     2  \n",
       "9997  why the hell would i want bing integrated into...     2  \n",
       "9998  and it overheats like a motherfucker mostly to...     2  \n",
       "9999  the same arguments apply to windows but we sti...     2  \n",
       "\n",
       "[10000 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat</th>\n      <th>txt</th>\n      <th>cleaned</th>\n      <th>ncat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sports</td>\n      <td>Barely better than Gabbert? He was significant...</td>\n      <td>barely better than gabbert he was significantl...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sports</td>\n      <td>Fuck the ducks and the Angels! But welcome to ...</td>\n      <td>fuck the ducks and the angels but welcome to a...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sports</td>\n      <td>Should have drafted more WRs.\\n\\n- Matt Millen...</td>\n      <td>should have drafted more wrs matt millen probably</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sports</td>\n      <td>[Done](https://i.imgur.com/2YZ90pm.jpg)</td>\n      <td>done https i imgur com  yz  pm jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sports</td>\n      <td>No!! NOO!!!!!</td>\n      <td>no noo</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>science_and_technology</td>\n      <td>Duuude, you're still on that ancient galaxy s3...</td>\n      <td>duuude you re still on that ancient galaxy s  ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>science_and_technology</td>\n      <td>&amp;gt; Moreover, if you buy an unlocked iPhone y...</td>\n      <td>gt moreover if you buy an unlocked iphone you...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>science_and_technology</td>\n      <td>Why the hell would I want Bing integrated into...</td>\n      <td>why the hell would i want bing integrated into...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>science_and_technology</td>\n      <td>And it overheats like a motherfucker, mostly t...</td>\n      <td>and it overheats like a motherfucker mostly to...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>science_and_technology</td>\n      <td>The same arguments apply to Windows but we sti...</td>\n      <td>the same arguments apply to windows but we sti...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows Ã— 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 161
    }
   ],
   "source": [
    "sample_cmnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unique category: 2\n"
     ]
    }
   ],
   "source": [
    "y_output = sample_cmnts['ncat']\n",
    "print(\"Unique category:\" ,y_output.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "metadata": {},
     "execution_count": 163
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "number_of_features = 2000\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features = number_of_features, stop_words=stopwords.words('english'))\n",
    "\n",
    "X_features = tfidf.fit_transform(sample_cmnts['cleaned']).toarray()\n",
    "X_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make test and training split (30:70)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train, y_test = train_test_split(X_features,y_output, random_state=0, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7000"
      ]
     },
     "metadata": {},
     "execution_count": 165
    }
   ],
   "source": [
    "X_train.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Neural Network\n",
    "network = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding fully connected input layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=100,\n",
    "                         activation='relu',\n",
    "                         input_shape=(number_of_features,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add fully connected layer with ReLU activation function\n",
    "network.add(layers.Dense(units=100, activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add fully connected layer with a softmax activation function\n",
    "network.add(layers.Dense(units=3, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile neural network\n",
    "network.compile(loss=\"categorical_crossentropy\", # Cross-entropy\n",
    "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"]) # Accuracy performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:532 train_step  **\n        loss = self.compiled_loss(\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:1527 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:4561 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:1117 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (100, 1) and (100, 3) are incompatible\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-33320acda68c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = network.fit(X_train, # Features\n\u001b[0m\u001b[1;32m      3\u001b[0m                       \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Three epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                       \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# No output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 505\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    506\u001b[0m             *args, **kwds))\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2655\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 2657\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:532 train_step  **\n        loss = self.compiled_loss(\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:1527 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:4561 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:1117 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (100, 1) and (100, 3) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# Train neural network\n",
    "history = network.fit(X_train, # Features\n",
    "                      y_train, # Target\n",
    "                      epochs=3, # Three epochs\n",
    "                      verbose=0, # No output\n",
    "                      batch_size=100, # Number of observations per batch\n",
    "                      validation_data=(X_test, y_test)) # Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "matrix([[0.07881104, 0.07881104, 0.07881104, 0.31524416, 0.07881104,\n",
       "         0.07881104, 0.07881104, 0.07881104, 0.07881104, 0.31524416,\n",
       "         0.07881104, 0.07881104, 0.07881104, 0.07881104, 0.07881104,\n",
       "         0.07881104, 0.07881104, 0.07881104, 0.07881104, 0.15762208,\n",
       "         0.07881104, 0.07881104, 0.31524416, 0.15762208, 0.15762208,\n",
       "         0.15762208, 0.07881104, 0.07881104, 0.07881104, 0.07881104,\n",
       "         0.07881104, 0.07881104, 0.07881104, 0.15762208, 0.07881104,\n",
       "         0.15762208, 0.07881104, 0.07881104, 0.07881104, 0.07881104,\n",
       "         0.07881104, 0.07881104, 0.07881104, 0.07881104, 0.07881104,\n",
       "         0.07881104, 0.07881104, 0.07881104, 0.07881104, 0.07881104,\n",
       "         0.07881104, 0.07881104, 0.23643312, 0.07881104, 0.23643312,\n",
       "         0.07881104, 0.15762208, 0.07881104, 0.23643312, 0.07881104,\n",
       "         0.15762208, 0.07881104, 0.07881104, 0.15762208, 0.07881104]])"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "features = tfidf_matrix\n",
    "features.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  (0, 0)\t0.07881104062391006\n  (0, 34)\t0.07881104062391006\n  (0, 44)\t0.07881104062391006\n  (0, 56)\t0.15762208124782012\n  (0, 21)\t0.07881104062391006\n  (0, 48)\t0.07881104062391006\n  (0, 4)\t0.07881104062391006\n  (0, 23)\t0.15762208124782012\n  (0, 13)\t0.07881104062391006\n  (0, 37)\t0.07881104062391006\n  (0, 19)\t0.15762208124782012\n  (0, 38)\t0.07881104062391006\n  (0, 59)\t0.07881104062391006\n  (0, 8)\t0.07881104062391006\n  (0, 17)\t0.07881104062391006\n  (0, 28)\t0.07881104062391006\n  (0, 45)\t0.07881104062391006\n  (0, 33)\t0.15762208124782012\n  (0, 7)\t0.07881104062391006\n  (0, 42)\t0.07881104062391006\n  (0, 14)\t0.07881104062391006\n  (0, 26)\t0.07881104062391006\n  (0, 2)\t0.07881104062391006\n  (0, 31)\t0.07881104062391006\n  (0, 27)\t0.07881104062391006\n  :\t:\n  (0, 57)\t0.07881104062391006\n  (0, 3)\t0.31524416249564025\n  (0, 32)\t0.07881104062391006\n  (0, 16)\t0.07881104062391006\n  (0, 39)\t0.07881104062391006\n  (0, 11)\t0.07881104062391006\n  (0, 46)\t0.07881104062391006\n  (0, 41)\t0.07881104062391006\n  (0, 30)\t0.07881104062391006\n  (0, 20)\t0.07881104062391006\n  (0, 35)\t0.15762208124782012\n  (0, 1)\t0.07881104062391006\n  (0, 43)\t0.07881104062391006\n  (0, 40)\t0.07881104062391006\n  (0, 24)\t0.15762208124782012\n  (0, 51)\t0.07881104062391006\n  (0, 63)\t0.15762208124782012\n  (0, 55)\t0.07881104062391006\n  (0, 47)\t0.07881104062391006\n  (0, 58)\t0.23643312187173018\n  (0, 22)\t0.31524416249564025\n  (0, 18)\t0.07881104062391006\n  (0, 52)\t0.23643312187173018\n  (0, 9)\t0.31524416249564025\n  (0, 6)\t0.07881104062391006\n"
     ]
    }
   ],
   "source": [
    "# get the first vector out (for the first document) \n",
    "print(features[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\ndf = pd.DataFrame(features.T.todense(), index=tfidf.get_feature_names(),columns=[[\"sam123\"]]) \\ndf1 = df.sort_values([\\'sam123\\'], ascending=[True])\\ndf1\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "# place tf-idf values in a pandas data frame \n",
    "\n",
    "# features.T.todense()\n",
    "# tfidf.get_feature_names()\n",
    "'''\n",
    "df = pd.DataFrame(features.T.todense(), index=tfidf.get_feature_names(),columns=[[\"sam123\"]]) \n",
    "df1 = df.sort_values(['sam123'], ascending=[True])\n",
    "df1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}