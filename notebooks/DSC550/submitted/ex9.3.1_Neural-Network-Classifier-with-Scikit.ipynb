{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitdd2052ddce364b29b70b9ef2f4771a5e",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Using the multi-label classifier dataset from earlier exercises (categorized-comments.jsonl in the reddit folder), fit a neural network classifier using scikit-learn. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json library to read data in jsonl file\n",
    "import json\n",
    "#import pandas library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pandas version: 1.2.4\n"
     ]
    }
   ],
   "source": [
    "#check versions of packages\n",
    "print('pandas version:', pd.__version__)\n",
    "# print('numpy version:', np.__version__)\n",
    "# print('scikit-learn version:', sklearn.__version__)\n",
    "# print('NLTK version:', nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      cat                                                txt\n",
       "0  sports  Barely better than Gabbert? He was significant...\n",
       "1  sports  Fuck the ducks and the Angels! But welcome to ...\n",
       "2  sports  Should have drafted more WRs.\\n\\n- Matt Millen...\n",
       "3  sports            [Done](https://i.imgur.com/2YZ90pm.jpg)\n",
       "4  sports                                      No!! NOO!!!!!"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat</th>\n      <th>txt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sports</td>\n      <td>Barely better than Gabbert? He was significant...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sports</td>\n      <td>Fuck the ducks and the Angels! But welcome to ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sports</td>\n      <td>Should have drafted more WRs.\\n\\n- Matt Millen...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sports</td>\n      <td>[Done](https://i.imgur.com/2YZ90pm.jpg)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sports</td>\n      <td>No!! NOO!!!!!</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "#read in the data as a dataframe\n",
    "filename = \"/home/arindam/Documents/mygithub/bu_dsc/data/raw/categorized-comments.jsonl\"\n",
    "with open(filename, 'r') as f:\n",
    "    jsonl_list = list(f)\n",
    "\n",
    "list1 = []\n",
    "for obj in jsonl_list:\n",
    "    res = json.loads(obj)\n",
    "    list1.append(res)\n",
    "    \n",
    "comments = pd.DataFrame(list1)\n",
    "\n",
    "#display the first few rows of data\n",
    "comments.head()\n",
    "# len(list1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The dataframe has a dimension of: (606476, 2)\nIt has 606476 comments\n"
     ]
    }
   ],
   "source": [
    "# print the dimension of the dataframe\n",
    "print('The dataframe has a dimension of:',comments.shape)\n",
    "print('It has {} comments'.format(comments.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The target names are : ['sports' 'science_and_technology' 'video_games']\nThis shows that there are only 3 categories in the total dataset\n"
     ]
    }
   ],
   "source": [
    "print('The target names are :', comments['cat'].unique())\n",
    "print('This shows that there are only 3 categories in the total dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert text to lowercase and romove punctuation\n",
    "#define a function to clean the text\n",
    "# import the required libraries here\n",
    "#import regular expressions library\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Remove punctuations and special characters, makes lower case\n",
    "    Args: text\n",
    "    Output: text\n",
    "    \"\"\"\n",
    "    text=text.lower() #makes text lowercase\n",
    "    text=re.sub('\\\\d|\\\\W+|_',' ',text) #removes extra white space\n",
    "    text=re.sub('[^a-zA-Z]',\" \", text) #removes any non-alphabetic characters\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import word tokenizer from NLTK\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenize_text(txt):\n",
    "    \"\"\"\n",
    "    Takes in a sentence, tokenizes the words into a list,\n",
    "    \"\"\"\n",
    "    stop_words = stopwords.words('english')\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    tokens = tokenizer.tokenize(txt)\n",
    "    return [token for token in tokens if token not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply NLTK's PorterStemmer\n",
    "#define a function to stem the words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def porter_stem_text(token_list):\n",
    "\n",
    "    porter = PorterStemmer()\n",
    "    return (\" \".join (porter.stem(token) for token in token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          cat                                                txt  ncat\n",
       "0      sports  Barely better than Gabbert? He was significant...     1\n",
       "1      sports  Fuck the ducks and the Angels! But welcome to ...     1\n",
       "2      sports  Should have drafted more WRs.\\n\\n- Matt Millen...     1\n",
       "3      sports            [Done](https://i.imgur.com/2YZ90pm.jpg)     1\n",
       "4      sports                                      No!! NOO!!!!!     1\n",
       "...       ...                                                ...   ...\n",
       "49995  sports  Florida State\\nAlabama\\nMichigan\\nAlabama\\nAla...     1\n",
       "49996  sports                                        Never does.     1\n",
       "49997  sports  I think Tosh is our best one left. Napier is g...     1\n",
       "49998  sports      Close, and we could really use a tight end...     1\n",
       "49999  sports  Yeah but that didn't really end all that well ...     1\n",
       "\n",
       "[50000 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat</th>\n      <th>txt</th>\n      <th>ncat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sports</td>\n      <td>Barely better than Gabbert? He was significant...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sports</td>\n      <td>Fuck the ducks and the Angels! But welcome to ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sports</td>\n      <td>Should have drafted more WRs.\\n\\n- Matt Millen...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sports</td>\n      <td>[Done](https://i.imgur.com/2YZ90pm.jpg)</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sports</td>\n      <td>No!! NOO!!!!!</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49995</th>\n      <td>sports</td>\n      <td>Florida State\\nAlabama\\nMichigan\\nAlabama\\nAla...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>49996</th>\n      <td>sports</td>\n      <td>Never does.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>49997</th>\n      <td>sports</td>\n      <td>I think Tosh is our best one left. Napier is g...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>49998</th>\n      <td>sports</td>\n      <td>Close, and we could really use a tight end...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>49999</th>\n      <td>sports</td>\n      <td>Yeah but that didn't really end all that well ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>50000 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Testing the functions\n",
    "# Taking a sample of the dataset\n",
    "sample_cmnts = comments[:50000]\n",
    "# txt = \"barely than significantly especially is an the ? better surrounded.\"\n",
    "# sample_cmnts['cat'].unique()\n",
    "# creating a dictionary to replace the string values to numeric\n",
    "d = {'sports':1,'science_and_technology':2,'video_games':3}\n",
    "sample_cmnts['ncat'] = sample_cmnts['cat'].map(d)\n",
    "sample_cmnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          cat                                                txt  ncat  \\\n",
       "0      sports  Barely better than Gabbert? He was significant...     1   \n",
       "1      sports  Fuck the ducks and the Angels! But welcome to ...     1   \n",
       "2      sports  Should have drafted more WRs.\\n\\n- Matt Millen...     1   \n",
       "3      sports            [Done](https://i.imgur.com/2YZ90pm.jpg)     1   \n",
       "4      sports                                      No!! NOO!!!!!     1   \n",
       "...       ...                                                ...   ...   \n",
       "49995  sports  Florida State\\nAlabama\\nMichigan\\nAlabama\\nAla...     1   \n",
       "49996  sports                                        Never does.     1   \n",
       "49997  sports  I think Tosh is our best one left. Napier is g...     1   \n",
       "49998  sports      Close, and we could really use a tight end...     1   \n",
       "49999  sports  Yeah but that didn't really end all that well ...     1   \n",
       "\n",
       "                                                 cleaned  \\\n",
       "0      barely better than gabbert he was significantl...   \n",
       "1      fuck the ducks and the angels but welcome to a...   \n",
       "2      should have drafted more wrs matt millen probably   \n",
       "3                    done https i imgur com  yz  pm jpg    \n",
       "4                                                no noo    \n",
       "...                                                  ...   \n",
       "49995     florida state alabama michigan alabama alabama   \n",
       "49996                                        never does    \n",
       "49997  i think tosh is our best one left napier is go...   \n",
       "49998         close and we could really use a tight end    \n",
       "49999  yeah but that didn t really end all that well ...   \n",
       "\n",
       "                                               tokenized  \\\n",
       "0      [barely, better, gabbert, significantly, bette...   \n",
       "1      [fuck, ducks, angels, welcome, new, niners, fans]   \n",
       "2                 [drafted, wrs, matt, millen, probably]   \n",
       "3                 [done, https, imgur, com, yz, pm, jpg]   \n",
       "4                                                  [noo]   \n",
       "...                                                  ...   \n",
       "49995  [florida, state, alabama, michigan, alabama, a...   \n",
       "49996                                            [never]   \n",
       "49997  [think, tosh, best, one, left, napier, gone, c...   \n",
       "49998            [close, could, really, use, tight, end]   \n",
       "49999                          [yeah, really, end, well]   \n",
       "\n",
       "                                                 stemmed  \n",
       "0      bare better gabbert significantli better year ...  \n",
       "1                   fuck duck angel welcom new niner fan  \n",
       "2                           draft wr matt millen probabl  \n",
       "3                          done http imgur com yz pm jpg  \n",
       "4                                                    noo  \n",
       "...                                                  ...  \n",
       "49995     florida state alabama michigan alabama alabama  \n",
       "49996                                              never  \n",
       "49997  think tosh best one left napier gone cristob g...  \n",
       "49998                   close could realli use tight end  \n",
       "49999                               yeah realli end well  \n",
       "\n",
       "[50000 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat</th>\n      <th>txt</th>\n      <th>ncat</th>\n      <th>cleaned</th>\n      <th>tokenized</th>\n      <th>stemmed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sports</td>\n      <td>Barely better than Gabbert? He was significant...</td>\n      <td>1</td>\n      <td>barely better than gabbert he was significantl...</td>\n      <td>[barely, better, gabbert, significantly, bette...</td>\n      <td>bare better gabbert significantli better year ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sports</td>\n      <td>Fuck the ducks and the Angels! But welcome to ...</td>\n      <td>1</td>\n      <td>fuck the ducks and the angels but welcome to a...</td>\n      <td>[fuck, ducks, angels, welcome, new, niners, fans]</td>\n      <td>fuck duck angel welcom new niner fan</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sports</td>\n      <td>Should have drafted more WRs.\\n\\n- Matt Millen...</td>\n      <td>1</td>\n      <td>should have drafted more wrs matt millen probably</td>\n      <td>[drafted, wrs, matt, millen, probably]</td>\n      <td>draft wr matt millen probabl</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sports</td>\n      <td>[Done](https://i.imgur.com/2YZ90pm.jpg)</td>\n      <td>1</td>\n      <td>done https i imgur com  yz  pm jpg</td>\n      <td>[done, https, imgur, com, yz, pm, jpg]</td>\n      <td>done http imgur com yz pm jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sports</td>\n      <td>No!! NOO!!!!!</td>\n      <td>1</td>\n      <td>no noo</td>\n      <td>[noo]</td>\n      <td>noo</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49995</th>\n      <td>sports</td>\n      <td>Florida State\\nAlabama\\nMichigan\\nAlabama\\nAla...</td>\n      <td>1</td>\n      <td>florida state alabama michigan alabama alabama</td>\n      <td>[florida, state, alabama, michigan, alabama, a...</td>\n      <td>florida state alabama michigan alabama alabama</td>\n    </tr>\n    <tr>\n      <th>49996</th>\n      <td>sports</td>\n      <td>Never does.</td>\n      <td>1</td>\n      <td>never does</td>\n      <td>[never]</td>\n      <td>never</td>\n    </tr>\n    <tr>\n      <th>49997</th>\n      <td>sports</td>\n      <td>I think Tosh is our best one left. Napier is g...</td>\n      <td>1</td>\n      <td>i think tosh is our best one left napier is go...</td>\n      <td>[think, tosh, best, one, left, napier, gone, c...</td>\n      <td>think tosh best one left napier gone cristob g...</td>\n    </tr>\n    <tr>\n      <th>49998</th>\n      <td>sports</td>\n      <td>Close, and we could really use a tight end...</td>\n      <td>1</td>\n      <td>close and we could really use a tight end</td>\n      <td>[close, could, really, use, tight, end]</td>\n      <td>close could realli use tight end</td>\n    </tr>\n    <tr>\n      <th>49999</th>\n      <td>sports</td>\n      <td>Yeah but that didn't really end all that well ...</td>\n      <td>1</td>\n      <td>yeah but that didn t really end all that well ...</td>\n      <td>[yeah, really, end, well]</td>\n      <td>yeah realli end well</td>\n    </tr>\n  </tbody>\n</table>\n<p>50000 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# Cleaning and tokenizing the texts in the comments\n",
    "# Using the transformed column for the model\n",
    "\n",
    "sample_cmnts['cleaned']=sample_cmnts['txt'].apply(clean_text)\n",
    "sample_cmnts['tokenized']=sample_cmnts['cleaned'].apply(tokenize_text)\n",
    "sample_cmnts['stemmed']=sample_cmnts['tokenized'].apply(porter_stem_text)\n",
    "sample_cmnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                cat                                                txt  ncat\n",
       "0            sports  Barely better than Gabbert? He was significant...     1\n",
       "1            sports  Fuck the ducks and the Angels! But welcome to ...     1\n",
       "2            sports  Should have drafted more WRs.\\n\\n- Matt Millen...     1\n",
       "3            sports            [Done](https://i.imgur.com/2YZ90pm.jpg)     1\n",
       "4            sports                                      No!! NOO!!!!!     1\n",
       "...             ...                                                ...   ...\n",
       "606471  video_games             No. It's probably only happened to you     3\n",
       "606472  video_games  I think most of the disappointment came from t...     3\n",
       "606473  video_games  dishonored 1/2 looked like arse, so what the h...     3\n",
       "606474  video_games                                          [removed]     3\n",
       "606475  video_games  I wish more games provided options like Rise o...     3\n",
       "\n",
       "[606476 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat</th>\n      <th>txt</th>\n      <th>ncat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sports</td>\n      <td>Barely better than Gabbert? He was significant...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sports</td>\n      <td>Fuck the ducks and the Angels! But welcome to ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sports</td>\n      <td>Should have drafted more WRs.\\n\\n- Matt Millen...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sports</td>\n      <td>[Done](https://i.imgur.com/2YZ90pm.jpg)</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sports</td>\n      <td>No!! NOO!!!!!</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>606471</th>\n      <td>video_games</td>\n      <td>No. It's probably only happened to you</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>606472</th>\n      <td>video_games</td>\n      <td>I think most of the disappointment came from t...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>606473</th>\n      <td>video_games</td>\n      <td>dishonored 1/2 looked like arse, so what the h...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>606474</th>\n      <td>video_games</td>\n      <td>[removed]</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>606475</th>\n      <td>video_games</td>\n      <td>I wish more games provided options like Rise o...</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>606476 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "comments['cat'].unique()\n",
    "# creating a dictionary to replace the string values to numeric\n",
    "d = {'sports':1,'science_and_technology':2,'video_games':3}\n",
    "comments['ncat'] = comments['cat'].map(d)\n",
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the target name\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Creating the features from the data set\n",
    "features, target = sample_cmnts.stemmed, sample_cmnts.ncat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Features-Training Set:  40000\nFeatures-Test Set:  10000\nTarget: Training Set:  40000\nTarget: Test Set:  10000\n"
     ]
    }
   ],
   "source": [
    "# Make test and training split (20:80)\n",
    "features_train,features_test,target_train, target_test = train_test_split(features,target, random_state=0, test_size = 0.2)\n",
    "\n",
    "print('Features-Training Set: ',len(features_train))\n",
    "print('Features-Test Set: ',len(features_test))\n",
    "print('Target: Training Set: ',len(target_train))\n",
    "print('Target: Test Set: ',len(target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "classifier = Pipeline([('vect', CountVectorizer()),\n",
    "                       ('tfidf', TfidfTransformer()),\n",
    "                       ('ann', MLPClassifier(hidden_layer_sizes=[500,150,100],  max_iter=20, activation='relu',solver='adam',verbose=False))\n",
    "])\n",
    "clf = classifier.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['/home/arindam/Documents/mygithub/bu_dsc/models/NN_classifier_sklearn.pkl']"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# Saving the Model\n",
    "\n",
    "import joblib\n",
    "from joblib import dump, load\n",
    "\n",
    "model_path=\"/home/arindam/Documents/mygithub/bu_dsc/models\"\n",
    "model_name=\"NN_classifier_sklearn.pkl\"\n",
    "filename = model_path + \"/\" + model_name \n",
    "# print(filename)\n",
    "joblib.dump(clf, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a saved model\n",
    "\n",
    "NN_clf = open(filename,'rb')\n",
    "\n",
    "clf1 = joblib.load(NN_clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the test set for the classifier\n",
    "y_pred = clf1.predict(features_test)\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Confusion Matrix\n================\n[[4302  691]\n [ 648 4359]]\nClassification Report\n=====================================================\n              precision    recall  f1-score   support\n\n           1       0.87      0.86      0.87      4993\n           2       0.86      0.87      0.87      5007\n\n    accuracy                           0.87     10000\n   macro avg       0.87      0.87      0.87     10000\nweighted avg       0.87      0.87      0.87     10000\n\nAccuracy Score\n=====\n0.8661\n"
     ]
    }
   ],
   "source": [
    "# Displaying the result metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(\"================\")\n",
    "print(confusion_matrix(target_test,y_pred))\n",
    "print(\"Classification Report\")\n",
    "print(\"=====================================================\")\n",
    "\n",
    "print(classification_report(target_test,y_pred))\n",
    "print(\"Accuracy Score\")\n",
    "print(\"=====\")\n",
    "\n",
    "print(accuracy_score(target_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}