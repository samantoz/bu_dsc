{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitdd2052ddce364b29b70b9ef2f4771a5e",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Using the multi-label classifier dataset from earlier exercises (categorized-comments.jsonl in the reddit folder), fit a neural network classifier using scikit-learn. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json library to read data in jsonl file\n",
    "import json\n",
    "#import pandas library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pandas version: 1.2.4\n"
     ]
    }
   ],
   "source": [
    "#check versions of packages\n",
    "print('pandas version:', pd.__version__)\n",
    "# print('numpy version:', np.__version__)\n",
    "# print('scikit-learn version:', sklearn.__version__)\n",
    "# print('NLTK version:', nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      cat                                                txt\n",
       "0  sports  Barely better than Gabbert? He was significant...\n",
       "1  sports  Fuck the ducks and the Angels! But welcome to ...\n",
       "2  sports  Should have drafted more WRs.\\n\\n- Matt Millen...\n",
       "3  sports            [Done](https://i.imgur.com/2YZ90pm.jpg)\n",
       "4  sports                                      No!! NOO!!!!!"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat</th>\n      <th>txt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sports</td>\n      <td>Barely better than Gabbert? He was significant...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sports</td>\n      <td>Fuck the ducks and the Angels! But welcome to ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sports</td>\n      <td>Should have drafted more WRs.\\n\\n- Matt Millen...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sports</td>\n      <td>[Done](https://i.imgur.com/2YZ90pm.jpg)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sports</td>\n      <td>No!! NOO!!!!!</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "#read in the data as a dataframe\n",
    "filename = \"/home/arindam/Documents/mygithub/bu_dsc/data/raw/categorized-comments.jsonl\"\n",
    "with open(filename, 'r') as f:\n",
    "    jsonl_list = list(f)\n",
    "\n",
    "list1 = []\n",
    "for obj in jsonl_list:\n",
    "    res = json.loads(obj)\n",
    "    list1.append(res)\n",
    "    \n",
    "comments = pd.DataFrame(list1)\n",
    "\n",
    "#display the first few rows of data\n",
    "comments.head()\n",
    "# len(list1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The dataframe has a dimension of: (606476, 2)\nIt has 606476 comments\n"
     ]
    }
   ],
   "source": [
    "# print the dimension of the dataframe\n",
    "print('The dataframe has a dimension of:',comments.shape)\n",
    "print('It has {} comments'.format(comments.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The target names are : ['sports' 'science_and_technology' 'video_games']\nThis shows that there are only 3 categories in the total dataset\n"
     ]
    }
   ],
   "source": [
    "print('The target names are :', comments['cat'].unique())\n",
    "print('This shows that there are only 3 categories in the total dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert text to lowercase and romove punctuation\n",
    "#define a function to clean the text\n",
    "# import the required libraries here\n",
    "#import regular expressions library\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Remove punctuations and special characters, makes lower case\n",
    "    Args: text\n",
    "    Output: text\n",
    "    \"\"\"\n",
    "    text=text.lower() #makes text lowercase\n",
    "    text=re.sub('\\\\d|\\\\W+|_',' ',text) #removes extra white space\n",
    "    text=re.sub('[^a-zA-Z]',\" \", text) #removes any non-alphabetic characters\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import word tokenizer from NLTK\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenize_text(txt):\n",
    "    \"\"\"\n",
    "    Takes in a sentence, tokenizes the words into a list,\n",
    "    \"\"\"\n",
    "    stop_words = stopwords.words('english')\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    tokens = tokenizer.tokenize(txt)\n",
    "    return [token for token in tokens if token not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply NLTK's PorterStemmer\n",
    "#define a function to stem the words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def porter_stem_text(token_list):\n",
    "\n",
    "    porter = PorterStemmer()\n",
    "    return (\" \".join (porter.stem(token) for token in token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                         cat  \\\n",
       "0                     sports   \n",
       "1                     sports   \n",
       "2                     sports   \n",
       "3                     sports   \n",
       "4                     sports   \n",
       "...                      ...   \n",
       "4995  science_and_technology   \n",
       "4996  science_and_technology   \n",
       "4997  science_and_technology   \n",
       "4998  science_and_technology   \n",
       "4999  science_and_technology   \n",
       "\n",
       "                                                    txt  ncat  \n",
       "0     Barely better than Gabbert? He was significant...     1  \n",
       "1     Fuck the ducks and the Angels! But welcome to ...     1  \n",
       "2     Should have drafted more WRs.\\n\\n- Matt Millen...     1  \n",
       "3               [Done](https://i.imgur.com/2YZ90pm.jpg)     1  \n",
       "4                                         No!! NOO!!!!!     1  \n",
       "...                                                 ...   ...  \n",
       "4995  I just recently bought a tv with chromecast bu...     2  \n",
       "4996  SMS just works, is reliable. Everyone I know h...     2  \n",
       "4997                                          [deleted]     2  \n",
       "4998  It's not perfect, but certainly better than th...     2  \n",
       "4999  I have the same issue.  So if you're not using...     2  \n",
       "\n",
       "[5000 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat</th>\n      <th>txt</th>\n      <th>ncat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sports</td>\n      <td>Barely better than Gabbert? He was significant...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sports</td>\n      <td>Fuck the ducks and the Angels! But welcome to ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sports</td>\n      <td>Should have drafted more WRs.\\n\\n- Matt Millen...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sports</td>\n      <td>[Done](https://i.imgur.com/2YZ90pm.jpg)</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sports</td>\n      <td>No!! NOO!!!!!</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4995</th>\n      <td>science_and_technology</td>\n      <td>I just recently bought a tv with chromecast bu...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4996</th>\n      <td>science_and_technology</td>\n      <td>SMS just works, is reliable. Everyone I know h...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4997</th>\n      <td>science_and_technology</td>\n      <td>[deleted]</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4998</th>\n      <td>science_and_technology</td>\n      <td>It's not perfect, but certainly better than th...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4999</th>\n      <td>science_and_technology</td>\n      <td>I have the same issue.  So if you're not using...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>5000 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Testing the functions\n",
    "sample_cmnts = comments[:5000]\n",
    "# txt = \"barely than significantly especially is an the ? better surrounded.\"\n",
    "# sample_cmnts['cat'].unique()\n",
    "# creating a dictionary to replace the string values to numeric\n",
    "d = {'sports':1,'science_and_technology':2,'video_games':3}\n",
    "sample_cmnts['ncat'] = sample_cmnts['cat'].map(d)\n",
    "sample_cmnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                         cat  \\\n",
       "0                     sports   \n",
       "1                     sports   \n",
       "2                     sports   \n",
       "3                     sports   \n",
       "4                     sports   \n",
       "...                      ...   \n",
       "4995  science_and_technology   \n",
       "4996  science_and_technology   \n",
       "4997  science_and_technology   \n",
       "4998  science_and_technology   \n",
       "4999  science_and_technology   \n",
       "\n",
       "                                                    txt  ncat  \\\n",
       "0     Barely better than Gabbert? He was significant...     1   \n",
       "1     Fuck the ducks and the Angels! But welcome to ...     1   \n",
       "2     Should have drafted more WRs.\\n\\n- Matt Millen...     1   \n",
       "3               [Done](https://i.imgur.com/2YZ90pm.jpg)     1   \n",
       "4                                         No!! NOO!!!!!     1   \n",
       "...                                                 ...   ...   \n",
       "4995  I just recently bought a tv with chromecast bu...     2   \n",
       "4996  SMS just works, is reliable. Everyone I know h...     2   \n",
       "4997                                          [deleted]     2   \n",
       "4998  It's not perfect, but certainly better than th...     2   \n",
       "4999  I have the same issue.  So if you're not using...     2   \n",
       "\n",
       "                                                cleaned  \\\n",
       "0     barely better than gabbert he was significantl...   \n",
       "1     fuck the ducks and the angels but welcome to a...   \n",
       "2     should have drafted more wrs matt millen probably   \n",
       "3                   done https i imgur com  yz  pm jpg    \n",
       "4                                               no noo    \n",
       "...                                                 ...   \n",
       "4995  i just recently bought a tv with chromecast bu...   \n",
       "4996  sms just works is reliable everyone i know has...   \n",
       "4997                                           deleted    \n",
       "4998  it s not perfect but certainly better than the...   \n",
       "4999  i have the same issue so if you re not using t...   \n",
       "\n",
       "                                              tokenized  \\\n",
       "0     [barely, better, gabbert, significantly, bette...   \n",
       "1     [fuck, ducks, angels, welcome, new, niners, fans]   \n",
       "2                [drafted, wrs, matt, millen, probably]   \n",
       "3                [done, https, imgur, com, yz, pm, jpg]   \n",
       "4                                                 [noo]   \n",
       "...                                                 ...   \n",
       "4995  [recently, bought, tv, chromecast, built, idea...   \n",
       "4996  [sms, works, reliable, everyone, know, sms, re...   \n",
       "4997                                          [deleted]   \n",
       "4998     [perfect, certainly, better, official, design]   \n",
       "4999  [issue, using, textra, another, messaging, app...   \n",
       "\n",
       "                                                stemmed  \n",
       "0     bare better gabbert significantli better year ...  \n",
       "1                  fuck duck angel welcom new niner fan  \n",
       "2                          draft wr matt millen probabl  \n",
       "3                         done http imgur com yz pm jpg  \n",
       "4                                                   noo  \n",
       "...                                                 ...  \n",
       "4995  recent bought tv chromecast built idea cast vl...  \n",
       "4996  sm work reliabl everyon know sm reach swiftli ...  \n",
       "4997                                              delet  \n",
       "4998             perfect certainli better offici design  \n",
       "4999   issu use textra anoth messag app get around issu  \n",
       "\n",
       "[5000 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat</th>\n      <th>txt</th>\n      <th>ncat</th>\n      <th>cleaned</th>\n      <th>tokenized</th>\n      <th>stemmed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sports</td>\n      <td>Barely better than Gabbert? He was significant...</td>\n      <td>1</td>\n      <td>barely better than gabbert he was significantl...</td>\n      <td>[barely, better, gabbert, significantly, bette...</td>\n      <td>bare better gabbert significantli better year ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sports</td>\n      <td>Fuck the ducks and the Angels! But welcome to ...</td>\n      <td>1</td>\n      <td>fuck the ducks and the angels but welcome to a...</td>\n      <td>[fuck, ducks, angels, welcome, new, niners, fans]</td>\n      <td>fuck duck angel welcom new niner fan</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sports</td>\n      <td>Should have drafted more WRs.\\n\\n- Matt Millen...</td>\n      <td>1</td>\n      <td>should have drafted more wrs matt millen probably</td>\n      <td>[drafted, wrs, matt, millen, probably]</td>\n      <td>draft wr matt millen probabl</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sports</td>\n      <td>[Done](https://i.imgur.com/2YZ90pm.jpg)</td>\n      <td>1</td>\n      <td>done https i imgur com  yz  pm jpg</td>\n      <td>[done, https, imgur, com, yz, pm, jpg]</td>\n      <td>done http imgur com yz pm jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sports</td>\n      <td>No!! NOO!!!!!</td>\n      <td>1</td>\n      <td>no noo</td>\n      <td>[noo]</td>\n      <td>noo</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4995</th>\n      <td>science_and_technology</td>\n      <td>I just recently bought a tv with chromecast bu...</td>\n      <td>2</td>\n      <td>i just recently bought a tv with chromecast bu...</td>\n      <td>[recently, bought, tv, chromecast, built, idea...</td>\n      <td>recent bought tv chromecast built idea cast vl...</td>\n    </tr>\n    <tr>\n      <th>4996</th>\n      <td>science_and_technology</td>\n      <td>SMS just works, is reliable. Everyone I know h...</td>\n      <td>2</td>\n      <td>sms just works is reliable everyone i know has...</td>\n      <td>[sms, works, reliable, everyone, know, sms, re...</td>\n      <td>sm work reliabl everyon know sm reach swiftli ...</td>\n    </tr>\n    <tr>\n      <th>4997</th>\n      <td>science_and_technology</td>\n      <td>[deleted]</td>\n      <td>2</td>\n      <td>deleted</td>\n      <td>[deleted]</td>\n      <td>delet</td>\n    </tr>\n    <tr>\n      <th>4998</th>\n      <td>science_and_technology</td>\n      <td>It's not perfect, but certainly better than th...</td>\n      <td>2</td>\n      <td>it s not perfect but certainly better than the...</td>\n      <td>[perfect, certainly, better, official, design]</td>\n      <td>perfect certainli better offici design</td>\n    </tr>\n    <tr>\n      <th>4999</th>\n      <td>science_and_technology</td>\n      <td>I have the same issue.  So if you're not using...</td>\n      <td>2</td>\n      <td>i have the same issue so if you re not using t...</td>\n      <td>[issue, using, textra, another, messaging, app...</td>\n      <td>issu use textra anoth messag app get around issu</td>\n    </tr>\n  </tbody>\n</table>\n<p>5000 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# Cleaning and tokenizing the texts in the comments\n",
    "# Using the transformed column for the model\n",
    "\n",
    "sample_cmnts['cleaned']=sample_cmnts['txt'].apply(clean_text)\n",
    "sample_cmnts['tokenized']=sample_cmnts['cleaned'].apply(tokenize_text)\n",
    "sample_cmnts['stemmed']=sample_cmnts['tokenized'].apply(porter_stem_text)\n",
    "sample_cmnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                cat                                                txt  ncat\n",
       "0            sports  Barely better than Gabbert? He was significant...     1\n",
       "1            sports  Fuck the ducks and the Angels! But welcome to ...     1\n",
       "2            sports  Should have drafted more WRs.\\n\\n- Matt Millen...     1\n",
       "3            sports            [Done](https://i.imgur.com/2YZ90pm.jpg)     1\n",
       "4            sports                                      No!! NOO!!!!!     1\n",
       "...             ...                                                ...   ...\n",
       "606471  video_games             No. It's probably only happened to you     3\n",
       "606472  video_games  I think most of the disappointment came from t...     3\n",
       "606473  video_games  dishonored 1/2 looked like arse, so what the h...     3\n",
       "606474  video_games                                          [removed]     3\n",
       "606475  video_games  I wish more games provided options like Rise o...     3\n",
       "\n",
       "[606476 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat</th>\n      <th>txt</th>\n      <th>ncat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sports</td>\n      <td>Barely better than Gabbert? He was significant...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sports</td>\n      <td>Fuck the ducks and the Angels! But welcome to ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sports</td>\n      <td>Should have drafted more WRs.\\n\\n- Matt Millen...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sports</td>\n      <td>[Done](https://i.imgur.com/2YZ90pm.jpg)</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sports</td>\n      <td>No!! NOO!!!!!</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>606471</th>\n      <td>video_games</td>\n      <td>No. It's probably only happened to you</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>606472</th>\n      <td>video_games</td>\n      <td>I think most of the disappointment came from t...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>606473</th>\n      <td>video_games</td>\n      <td>dishonored 1/2 looked like arse, so what the h...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>606474</th>\n      <td>video_games</td>\n      <td>[removed]</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>606475</th>\n      <td>video_games</td>\n      <td>I wish more games provided options like Rise o...</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>606476 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "comments['cat'].unique()\n",
    "# creating a dictionary to replace the string values to numeric\n",
    "d = {'sports':1,'science_and_technology':2,'video_games':3}\n",
    "comments['ncat'] = comments['cat'].map(d)\n",
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the target name\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Creating the features from the data set\n",
    "features, target = sample_cmnts.stemmed, sample_cmnts.ncat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Features-Training Set:  4000\nFeatures-Test Set:  1000\nTarget: Training Set:  4000\nTarget: Test Set:  1000\n"
     ]
    }
   ],
   "source": [
    "# Make test and training split (20:80)\n",
    "features_train,features_test,target_train, target_test = train_test_split(features,target, random_state=0, test_size = 0.2)\n",
    "\n",
    "print('Features-Training Set: ',len(features_train))\n",
    "print('Features-Test Set: ',len(features_test))\n",
    "print('Target: Training Set: ',len(target_train))\n",
    "print('Target: Test Set: ',len(target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 0.58971857\n",
      "Iteration 2, loss = 0.37839883\n",
      "Iteration 3, loss = 0.18761950\n",
      "Iteration 4, loss = 0.09082355\n",
      "Iteration 5, loss = 0.06428572\n",
      "Iteration 6, loss = 0.05568206\n",
      "Iteration 7, loss = 0.05156994\n",
      "Iteration 8, loss = 0.04951650\n",
      "Iteration 9, loss = 0.05007860\n",
      "Iteration 10, loss = 0.05006858\n",
      "Iteration 11, loss = 0.04898457\n",
      "Iteration 12, loss = 0.04651422\n",
      "Iteration 13, loss = 0.04777093\n",
      "Iteration 14, loss = 0.04771221\n",
      "Iteration 15, loss = 0.04837100\n",
      "Iteration 16, loss = 0.04761992\n",
      "Iteration 17, loss = 0.04742211\n",
      "Iteration 18, loss = 0.04543751\n",
      "Iteration 19, loss = 0.04580425\n",
      "Iteration 20, loss = 0.04609289\n",
      "Iteration 21, loss = 0.04612687\n",
      "Iteration 22, loss = 0.04538598\n",
      "Iteration 23, loss = 0.04511230\n",
      "Iteration 24, loss = 0.04534189\n",
      "Iteration 25, loss = 0.04674193\n",
      "Iteration 26, loss = 0.04484787\n",
      "Iteration 27, loss = 0.04848498\n",
      "Iteration 28, loss = 0.04670607\n",
      "Iteration 29, loss = 0.04502825\n",
      "Iteration 30, loss = 0.04468233\n",
      "Iteration 31, loss = 0.04455459\n",
      "Iteration 32, loss = 0.04495841\n",
      "Iteration 33, loss = 0.04543680\n",
      "Iteration 34, loss = 0.04465475\n",
      "Iteration 35, loss = 0.04480961\n",
      "Iteration 36, loss = 0.04571275\n",
      "Iteration 37, loss = 0.04378223\n",
      "Iteration 38, loss = 0.04399635\n",
      "Iteration 39, loss = 0.04474187\n",
      "Iteration 40, loss = 0.04697187\n",
      "Iteration 41, loss = 0.04726350\n",
      "Iteration 42, loss = 0.04475479\n",
      "Iteration 43, loss = 0.04406919\n",
      "Iteration 44, loss = 0.04408125\n",
      "Iteration 45, loss = 0.04491291\n",
      "Iteration 46, loss = 0.04516228\n",
      "Iteration 47, loss = 0.04512977\n",
      "Iteration 48, loss = 0.04556770\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "classifier = Pipeline([('vect', CountVectorizer()),\n",
    "                       ('tfidf', TfidfTransformer()),\n",
    "                       ('ann', MLPClassifier(hidden_layer_sizes=[500,150], verbose=True))\n",
    "])\n",
    "clf = classifier.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 0.57647814\n",
      "Iteration 2, loss = 0.32844515\n",
      "Iteration 3, loss = 0.12928790\n",
      "Iteration 4, loss = 0.06885031\n",
      "Iteration 5, loss = 0.05745424\n",
      "Iteration 6, loss = 0.05211757\n",
      "Iteration 7, loss = 0.04830122\n",
      "Iteration 8, loss = 0.04998007\n",
      "Iteration 9, loss = 0.05045178\n",
      "Iteration 10, loss = 0.04989999\n",
      "Iteration 11, loss = 0.04849351\n",
      "Iteration 12, loss = 0.04775651\n",
      "Iteration 13, loss = 0.04593955\n",
      "Iteration 14, loss = 0.04610268\n",
      "Iteration 15, loss = 0.04722192\n",
      "Iteration 16, loss = 0.04523295\n",
      "Iteration 17, loss = 0.04622596\n",
      "Iteration 18, loss = 0.04493510\n",
      "Iteration 19, loss = 0.04676666\n",
      "Iteration 20, loss = 0.04523932\n",
      "Iteration 21, loss = 0.04700011\n",
      "Iteration 22, loss = 0.04746295\n",
      "Iteration 23, loss = 0.04408354\n",
      "Iteration 24, loss = 0.04436455\n",
      "Iteration 25, loss = 0.04499572\n",
      "Iteration 26, loss = 0.04465129\n",
      "Iteration 27, loss = 0.04425375\n",
      "Iteration 28, loss = 0.04419968\n",
      "Iteration 29, loss = 0.04418524\n",
      "Iteration 30, loss = 0.04554814\n",
      "Iteration 31, loss = 0.04416048\n",
      "Iteration 32, loss = 0.04370946\n",
      "Iteration 33, loss = 0.04507025\n",
      "Iteration 34, loss = 0.04482340\n",
      "Iteration 35, loss = 0.04391764\n",
      "Iteration 36, loss = 0.04463485\n",
      "Iteration 37, loss = 0.04449291\n",
      "Iteration 38, loss = 0.04397710\n",
      "Iteration 39, loss = 0.04427462\n",
      "Iteration 40, loss = 0.04389706\n",
      "Iteration 41, loss = 0.04393512\n",
      "Iteration 42, loss = 0.04363030\n",
      "Iteration 43, loss = 0.04424380\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "classifier = Pipeline([('vect', CountVectorizer()),\n",
    "                       ('tfidf', TfidfTransformer()),\n",
    "                       ('ann', MLPClassifier(hidden_layer_sizes=[500,150,100],  max_iter=50, activation='relu',solver='adam',verbose=True))\n",
    "])\n",
    "clf = classifier.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the test set for the classifier\n",
    "y_pred = clf.predict(features_test)\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[635  62]\n [ 71 232]]\n              precision    recall  f1-score   support\n\n           1       0.90      0.91      0.91       697\n           2       0.79      0.77      0.78       303\n\n    accuracy                           0.87      1000\n   macro avg       0.84      0.84      0.84      1000\nweighted avg       0.87      0.87      0.87      1000\n\n0.867\n"
     ]
    }
   ],
   "source": [
    "# Displaying the result metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "\n",
    "print(confusion_matrix(target_test,y_pred))\n",
    "print(classification_report(target_test,y_pred))\n",
    "print(accuracy_score(target_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}