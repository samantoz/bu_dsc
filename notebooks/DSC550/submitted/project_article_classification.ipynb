{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRISP - DM Method\n",
    "### [Business Understanding](#business_understanding)\n",
    "### [Data Understanding](#data_understanding)\n",
    "   #### [Step 1: Load data into a dataframe](#load_data_df)\n",
    "   #### [Step 2: Display the dimensions of the file](#display_data)\n",
    "   #### [Step 3: what type of variables are in the table](#variable_types)\n",
    "### Data Prep\n",
    "   #### [Adding new columns from datetime](#adding_yr_mnth)\n",
    "   #### [Milestone 2](#milestone_2)\n",
    "   #### [Test data preparation](#test_data)\n",
    "### Modeling\n",
    "### Evaluation\n",
    "### Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='business_understanding'></a>\n",
    "## 1. Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This dataset contains around 200k news headlines from the year 2012 to 2018 obtained from HuffPost. The model trained on this dataset could be used to identify tags for untracked news articles or to identify the type of language used in different news articles.\n",
    "- Can you categorize news articles based on their headlines and short descriptions?\n",
    "- Do news articles from different categories have different writing styles?\n",
    "- A classifier trained on this dataset could be used on a free text to identify the type of language being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_understanding'></a>\n",
    "## 2. Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping / Classification of documents based on Text Analysis - Part 1 \n",
    "## Graphics Analysis\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check versions of packages\n",
    "print('Python version:')  \n",
    "!python --version\n",
    "print('pandas version:', pd.__version__)\n",
    "print('numpy version:', np.__version__)\n",
    "# print('scikit-learn version:', sklearn.__version__)\n",
    "# print('NLTK version:', nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the parameters for the pandas dataframe\n",
    "\n",
    "output_width = 1000\n",
    "#output_width = 80 #//*** Normal Output width\n",
    "pd.set_option(\"display.width\", output_width)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load_data_df'></a>\n",
    "### Step 1: Load data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset contains the topics already marked and so I would like to use this \n",
    "# as training set for my model.\n",
    "# I have a separate test data set for testing.\n",
    "\n",
    "filename = \"~/Documents/mygithub/bu_dsc/data/external/News_Category_Dataset_v2.json\"\n",
    "test_file = \"~/Documents/mygithub/bu_dsc/data/external/global-issues.csv\"\n",
    "\n",
    "df_all = pd.read_json(filename, lines = True)\n",
    "#display the first few rows of data\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='display_data'></a>\n",
    "### Step 2: Display the dimensions of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## so you’ll have a good idea the amount of data you are working with.\n",
    "print(\"The dimension of the table is: \", df_all.shape)\n",
    "print(\"Checking to see if there are any missing data: \")\n",
    "df_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='variable_types'></a>\n",
    "### Step 3: what type of variables are in the table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at summary information about your data (total, mean, min, max, freq, unique, etc.) \n",
    "# Does this present any more questions for you? \n",
    "# Does it lead you to a conclusion yet?\n",
    "\n",
    "print(\"Describe Data\") \n",
    "print(df_all.describe()) \n",
    "print(\"Summarized Data\") \n",
    "print(df_all.describe(include='O'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='adding_yr_mnth'></a>\n",
    "### Breaking down the datetime column into year month for visualization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breaking down the date column to Year and month separate columns for easy analysis\n",
    "# the date column is having the data in the datetime format\n",
    "\n",
    "df_all['year'] = df_all['date'].apply(lambda x: x.year)\n",
    "df_all['month'] = df_all['date'].apply(lambda x: x.month)\n",
    "\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all.columns\n",
    "for col in df_all.columns[[0,6,7]]:\n",
    "    print(col,len(df_all[col].unique()),df_all[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dimensions of the dataframe post addition of the new columns\n",
    "# We now see that it has 2 more columns added towards the end as year and month of type integer\n",
    "\n",
    "print(\"The dimension of the table is: \", df_all.shape)\n",
    "print(\"Checking to see if there are any missing data: \")\n",
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barcharts: set up the figure size \n",
    "# This barchart shows the count of topics for each category.\n",
    "# Creating a separate dataframe for the categories in the training set\n",
    "# We can see that the Top 3 categories include Politics,Wellness and Entertainment and those are by far more than the rest.\n",
    "# %matplotlib inline \n",
    "\n",
    "# In the X axis we are showing article categories\n",
    "# The Y axis shows the corresponding counts \n",
    "width = 0.75 # the width of the bars\n",
    "\n",
    "df_catg = df_all.groupby(['category'])['category'].count()\n",
    "\n",
    "ax = df_catg.plot(kind='bar', figsize=(20,10), color=\"indigo\",width = width)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "ax.set_title('Category Count', fontsize=25) \n",
    "ax.set_xlabel('Category', fontsize=20)\n",
    "ax.set_ylabel('Counts', fontsize=20)\n",
    "ax.tick_params(axis='both', labelsize=15)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now subsetting the actual dataset\n",
    "# Subsetting the dataset by category and year\n",
    "\n",
    "subset_bycatgyear = df_all[['category','year']]\n",
    "# This is an array of the years in the dataset\n",
    "years = subset_bycatgyear['year'].unique()\n",
    "years\n",
    "# This shows an array of categories that is present in our dataset\n",
    "# categories = subset_bycatgyear['category'].unique()\n",
    "# categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a grouping subset to be used in my analysis later\n",
    "\n",
    "year_grp = subset_bycatgyear.groupby(['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the data\n",
    "# print('showing all the categories for a particular year:')\n",
    "# year_grp.get_group(2012)\n",
    "print('showing all the categories for a particular year:')\n",
    "year_grp['category'].value_counts().loc[2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe out of the grouping set\n",
    "\n",
    "df = pd.DataFrame(year_grp['category'].value_counts())\n",
    "df.unstack(level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the top 3 categories for each year\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped barcharts: set up the figure size \n",
    "# This barchart shows the count of topics for each category.\n",
    "# We can see that the Top 3 categories include Politics,Wellness and Entertainment and those are by far more than the rest.\n",
    "# %matplotlib inline \n",
    "\n",
    "# In the X axis we are showing article categories\n",
    "# The Y axis shows the corresponding counts \n",
    "width = 0.50 # the width of the bars\n",
    "plt.rcParams['figure.figsize'] = (20, 10)\n",
    "\n",
    "# Get the data\n",
    "# df = subset_bycatgyear.groupby(['year','category'])['category'].count()\n",
    "df1 = df.unstack(level=1)\n",
    "\n",
    "ax = df1.plot(kind='bar', width = width)\n",
    "\n",
    "# Define the bar\n",
    "\n",
    "\n",
    "# make the bar plot\n",
    "plt.xticks(rotation=0)\n",
    "ax.set_title('Category Count', fontsize=25) \n",
    "ax.set_xlabel('Year', fontsize=20)\n",
    "ax.set_ylabel('Counts', fontsize=20)\n",
    "# ax.tick_params(axis='both', labelsize=15)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now that you have created your idea, located data, and have started your graphical analysis, we need to shift to starting the dimensionality/feature reduction and feature engineering steps of the project.\n",
    "\n",
    "It is important to note that these are milestones, meant to keep you on track for the final project submission. At any point, you can pivot or modify your project as needed based on what you discover. These milestones are not final versions; they are drafts of the many steps you need to complete along the way.\n",
    "\n",
    "In Milestone 2, you should drop any features that are not useful for your model building. You should explain and justify why the feature dropped is not useful. You should address any missing data issues. Build any new features that you need for your model, e.g., create dummy variables for categorical features if necessary. Explain your process at each step. You can use any methods/tools you think are most appropriate. Again, keep in mind that this may look very different from what is done in the Titanic tutorial case study. You should do what makes sense for your project. Be careful to avoid data snooping in these steps.\n",
    "\n",
    "As a reminder – Teams is a great place to discuss your project with your peers. Feel free to solicit feedback/input (without creating a group project!) and collaborate on your projects with your peers.\n",
    "\n",
    "Each milestone will build on top of each other, so make sure you do not fall behind.\n",
    "\n",
    "Submit Milestone 2 as a PDF or Jupyter Notebook, along with any applicable code to the submission link.\n",
    "'''"
   ]
  },
  {
   "source": [
    "<a id='milestone_2'></a>\n",
    "## Milestone 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the first few rows of data\n",
    "df_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dimensions of the dataframe post addition of the new columns\n",
    "# We now see that it has 2 more columns added towards the end as year and month of type integer\n",
    "\n",
    "print(\"The dimension of the table is: \", df_all.shape)\n",
    "print(\"Checking to see if there are any missing data: \")\n",
    "df_all.info()"
   ]
  },
  {
   "source": [
    "### Description of the data set\n",
    "The training data set contains 200,853 rows and they represent the articles from huffington post on different topics.\n",
    "\n",
    "It has 8 columns with 7 features as the \"category\" column represents the target vector. We would be using this as a supervised learning algorithm.\n",
    "\n",
    "The model would be using the headline and the short description feature to categorize the headlines. \n",
    "\n",
    "The year and month features were added later and is a modified feature. So these 2 features could be dropped from the data set for model purpose. These 2 features would not add any additional information. So as part of the Feature selection process I am dropping them.\n",
    "\n",
    "The link feature shows the internet link of the headline. It shows a weblink to the news item and hence would not add any additional information for the categorization.\n",
    "\n",
    "As this project relates to text analysis so I am for the time being not considering any other features from the dataset other than the text values.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the next step I am creating a new training dataframe df_train and the target vector as y\n",
    "# \n",
    "df_train = df_all[['headline','short_description']]\n",
    "df_train.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The target vector \n",
    "y = df_all['category']\n",
    "print ('Length of the target vector: ' , len(y))\n",
    "print ('Size of the vector: ',y.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In the next section I am working on cleaning the dataset and word tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert text to lowercase and romove punctuation\n",
    "#define a function to clean the text\n",
    "\n",
    "# import the required libraries here\n",
    "\n",
    "#import regular expressions library\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Remove punctuations and special characters, makes lower case\n",
    "    Args: text\n",
    "    Output: text\n",
    "    \"\"\"\n",
    "    text=text.lower() #makes text lowercase\n",
    "    text=re.sub('\\\\d|\\\\W+|_',' ',text) #removes extra white space\n",
    "    text=re.sub('[^a-zA-Z]',\" \", text) #removes any non-alphabetic characters\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import word tokenizer from NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the functions\n",
    "input_txt = \"There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV\"\n",
    "output_txt = clean_text(input_txt)\n",
    "output_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new data frame for the column for each pre-processing step\n",
    "#take a random sample of the dataframe to cut down on processing time\n",
    "#number of comments to keep\n",
    "num_comments = 5\n",
    "df_sample_headline = df_train.sample(n = num_comments).reset_index(drop = True )\n",
    "df_sample_headline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying the cleaning for the headline part\n",
    "\n",
    "#apply text cleaning function\n",
    "df_sample_headline['hdln_clean'] = df_sample_headline['headline'].apply(clean_text)\n",
    "#apply tokenizing\n",
    "df_sample_headline['hdln_tokenized'] = df_sample_headline['hdln_clean'].apply(tokenize_text)\n",
    "#apply PorterStemmer function\n",
    "# df_sample['txt_stemmed'] = df_sample['txt_tokenized'].apply(stem_text)\n",
    "#put the text back together (untokenize)\n",
    "df_sample_headline['hdln_final'] = df_sample_headline['hdln_tokenized'].apply(lambda text: ' '.join(text))\n",
    "#view the pre-processed text\n",
    "print('Show the dimension of the new dataframe: ', df_sample_headline.shape)\n",
    "df_sample_headline.head()"
   ]
  },
  {
   "source": [
    "<a id='test_data'></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### I am loading the below dataset as the test dataset\n",
    "### Although this dataset as I downloaded from kaggle site did have some classification data but I will tend to avoid it and use it to predict the cluster\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(test_file)\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python38564bitbaseconda9c4f9ef913ec46fba5f8aa62969238f5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}