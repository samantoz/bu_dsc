{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitdd2052ddce364b29b70b9ef2f4771a5e",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Using the multi-label classifier dataset from earlier exercises (categorized-comments.jsonl in the reddit folder), fit a neural network classifier using scikit-learn. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json library to read data in jsonl file\n",
    "import json\n",
    "#import pandas library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check versions of packages\n",
    "print('pandas version:', pd.__version__)\n",
    "# print('numpy version:', np.__version__)\n",
    "# print('scikit-learn version:', sklearn.__version__)\n",
    "# print('NLTK version:', nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the data as a dataframe\n",
    "filename = \"/home/arindam/Documents/mygithub/bu_dsc/data/raw/categorized-comments.jsonl\"\n",
    "with open(filename, 'r') as f:\n",
    "    jsonl_list = list(f)\n",
    "\n",
    "list1 = []\n",
    "for obj in jsonl_list:\n",
    "    res = json.loads(obj)\n",
    "    list1.append(res)\n",
    "    \n",
    "comments = pd.DataFrame(list1)\n",
    "\n",
    "#display the first few rows of data\n",
    "comments.head()\n",
    "# len(list1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the dimension of the dataframe\n",
    "print('The dataframe has a dimension of:',comments.shape)\n",
    "print('It has {} comments'.format(comments.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The target names are :', comments['cat'].unique())\n",
    "print('This shows that there are only 3 categories in the total dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert text to lowercase and romove punctuation\n",
    "#define a function to clean the text\n",
    "# import the required libraries here\n",
    "#import regular expressions library\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Remove punctuations and special characters, makes lower case\n",
    "    Args: text\n",
    "    Output: text\n",
    "    \"\"\"\n",
    "    text=text.lower() #makes text lowercase\n",
    "    text=re.sub('\\\\d|\\\\W+|_',' ',text) #removes extra white space\n",
    "    text=re.sub('[^a-zA-Z]',\" \", text) #removes any non-alphabetic characters\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import word tokenizer from NLTK\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenize_text(txt):\n",
    "    \"\"\"\n",
    "    Takes in a sentence, tokenizes the words into a list,\n",
    "    \"\"\"\n",
    "    stop_words = stopwords.words('english')\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    tokens = tokenizer.tokenize(txt)\n",
    "    return [token for token in tokens if token not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply NLTK's PorterStemmer\n",
    "#define a function to stem the words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def porter_stem_text(token_list):\n",
    "\n",
    "    porter = PorterStemmer()\n",
    "    return (\" \".join (porter.stem(token) for token in token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply NLTK's WordNetLemmatizer\n",
    "#define a function to lemmatize the words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_text(token_list):\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return (\" \".join (lemmatizer.lemmatize(token) for token in token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the functions\n",
    "sample_cmnts = comments[:5000]\n",
    "# txt = \"barely than significantly especially is an the ? better surrounded.\"\n",
    "# sample_cmnts['cat'].unique()\n",
    "# creating a dictionary to replace the string values to numeric\n",
    "d = {'sports':1,'science_and_technology':2,'video_games':3}\n",
    "sample_cmnts['ncat'] = sample_cmnts['cat'].map(d)\n",
    "sample_cmnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cmnts['cleaned']=sample_cmnts['txt'].apply(clean_text)\n",
    "sample_cmnts['tokenized']=sample_cmnts['cleaned'].apply(tokenize_text)\n",
    "sample_cmnts['stemmed']=sample_cmnts['tokenized'].apply(porter_stem_text)\n",
    "sample_cmnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['cleaned']=comments['txt'].apply(clean_text)\n",
    "comments['tokenized']=comments['cleaned'].apply(tokenize_text)\n",
    "comments['stemmed']=comments['tokenized'].apply(porter_stem_text)\n",
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['cat'].unique()\n",
    "# creating a dictionary to replace the string values to numeric\n",
    "d = {'sports':1,'science_and_technology':2,'video_games':3}\n",
    "comments['ncat'] = comments['cat'].map(d)\n",
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the traget name\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Creating the features from the data set\n",
    "features, target = sample_cmnts.stemmed, sample_cmnts.ncat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make test and training split\n",
    "features_train,features_test,target_train, target_test = train_test_split(features,target, random_state=0, test_size = 0.2)\n",
    "\n",
    "print('Features-Training Set: ',len(features_train))\n",
    "print('Features-Test Set: ',len(features_test))\n",
    "print('Target: Training Set: ',len(target_train))\n",
    "print('Target: Test Set: ',len(target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features from text files\n",
    "'''\n",
    "Text files are actually series of words(ordered). In order to run ML algorithms we need to convert the text files into numerical feature vectors. We will use \"bag of words\" model.\n",
    "Each unique word in our dictionary will correspond to a feature.\n",
    "'''\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_features_train = count_vect.fit_transform(features_train)\n",
    "'''\n",
    "Using count_vect.fit_transform, we are learning the vocabulary dictionary and it returns a Document-Term matrix\n",
    "'''\n",
    "# X_features_train.shape\n",
    "print('Shape of the feature set:', X_features_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TF (Term Frequency) helps in avoiding the issue with giving more weight to longer documents than shorter documents. count(words) / Total words (in each document)\n",
    "TF-IDF even reduces the weightage of more common words in documents (e.g., the, is an etc)\n",
    "'''\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_features_train_tfidf = tfidf_transformer.fit_transform(X_features_train)\n",
    "print('Shape of the tfidf feature matrix:', X_features_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature=pd.DataFrame(\n",
    "    y_target_train.todense(),\n",
    "    columns = count_vect.get_feature_names()\n",
    ")\n",
    "df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "regressor = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('ann', MLPRegressor(hidden_layer_sizes=[500,150], verbose=True)),\n",
    "                      ])\n",
    "regressor = regressor.fit(features_train, target_train)\n",
    "# predicted_regressor = regressor.predict(features_train)\n",
    "# np.mean(predicted_regressor == target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Pipeline([('vect', CountVectorizer()),\n",
    "                       ('tfidf', TfidfTransformer()),\n",
    "                       ('ann', MLPClassifier(hidden_layer_sizes=[500,150], verbose=True))\n",
    "])\n",
    "clf = classifier.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the test set\n",
    "y_pred = regressor.predict(features_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the test set for the classifier\n",
    "y_pred = clf.predict(features_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the result metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "\n",
    "print(confusion_matrix(target_test,y_pred))\n",
    "print(classification_report(target_test,y_pred))\n",
    "print(accuracy_score(target_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}