{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitbaseconda9c4f9ef913ec46fba5f8aa62969238f5",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "import warnings\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#9.1 reducing features using Principal Components\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "print(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(digits.images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features= StandardScaler().fit_transform(digits.data)\n",
    "pca=PCA(n_components=0.99, whiten=True)\n",
    "features_pca = pca.fit_transform(features)\n",
    "print(\"original number of features:\", features.shape[1])\n",
    "print(\"reduced number of features:\", features_pca.shape[1])\n",
    "print(\"output from 9.1 done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.4 Reducing Features Using Matrix Factorization\n",
    "features = digits.data\n",
    "nmf=NMF(n_components=10, random_state=1)\n",
    "features_nmf=nmf.fit_transform(features)\n",
    "print(\"Original number of features:\", features.shape[1])\n",
    "print(\"reduced number of features:\", features_nmf.shape[1])\n",
    "print(\"output from 9.4 done!\")\n",
    "\n"
   ]
  },
  {
   "source": [
    "Variance thresholding (VT) is one of the most basic approaches to feature selection. It\n",
    "is motivated by the idea that features with low variance are likely less interesting (and\n",
    "useful) than features with high variance.\n",
    "First, the variance is not centered; that is, it is in the squared unit of the feature itself.So the VT will not work when feature sets contain different units (e.g., one feature is in years while a different feature is in dollars).\n",
    "Second, the variance threshold is selected manually, so we have to use our own judgment for a good value to select"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10.1 - Thresholding Numerical Feature Variance\n",
    "\n",
    "#import data\n",
    "iris= datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create features and target\n",
    "features=iris.data\n",
    "target=iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create thresholder\n",
    "thresholder = VarianceThreshold(threshold=.5)\n",
    "\n",
    "#create high variance feature matrix and print\n",
    "features_high_variance=thresholder.fit_transform(features)\n",
    "print(features_high_variance[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the variances\n",
    "thresholder.fit(features).variances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below process shows how to standardise the feature variances\n",
    "# Standardize feature matrix\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "features_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate variance of each feature\n",
    "selector = VarianceThreshold()\n",
    "selector.fit(features_std).variances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10.2 - Thresholding Binary Feature Variance\n",
    "# Binary categorical features\n",
    "\n",
    "features = [[0,1,0],\n",
    "            [0,1,1],\n",
    "            [0,1,0],\n",
    "            [0,1,1],\n",
    "            [1,0,0]]\n",
    "# Run thresholder by variance\n",
    "thresholder=VarianceThreshold(threshold = (.75*(1-.75)))\n",
    "print(thresholder.fit_transform(features))\n"
   ]
  }
 ]
}