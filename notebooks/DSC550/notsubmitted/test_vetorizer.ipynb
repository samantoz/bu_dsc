{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitbaseconda9c4f9ef913ec46fba5f8aa62969238f5",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  Day of Week                                        comments\n",
       "0      Monday                             Hello, how are you?\n",
       "1     Tuesday                            Today is a good day!\n",
       "2   Wednesday  It's my birthday so it's a really special day!\n",
       "3    Thursday       Today is neither a good day or a bad day!\n",
       "4      Friday                           I'm having a bad day.\n",
       "5    Saturday       There' s nothing special happening today.\n",
       "6      Sunday                      Today is a SUPER good day!"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Day of Week</th>\n      <th>comments</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Monday</td>\n      <td>Hello, how are you?</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Tuesday</td>\n      <td>Today is a good day!</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Wednesday</td>\n      <td>It's my birthday so it's a really special day!</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Thursday</td>\n      <td>Today is neither a good day or a bad day!</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Friday</td>\n      <td>I'm having a bad day.</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Saturday</td>\n      <td>There' s nothing special happening today.</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Sunday</td>\n      <td>Today is a SUPER good day!</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename=\"DailyComments.csv\"\n",
    "dataframe_comments = pd.read_csv(filename)\n",
    "dataframe_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5791 entries, 0 to 5790\nData columns (total 2 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   Text       5791 non-null   object\n 1   Sentiment  5791 non-null   int64 \ndtypes: int64(1), object(1)\nmemory usage: 90.6+ KB\n"
     ]
    }
   ],
   "source": [
    "filename=\"stock_data.csv\"\n",
    "dataframe_stockdata = pd.read_csv(filename)\n",
    "dataframe_stockdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(dataframe_stockdata['Text'],dataframe_stockdata['Sentiment'],test_size=0.3,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3644              With some luck BT 22 is current target.\n",
       "3338    Automatic cuts starts today, european crisis g...\n",
       "4061    COX Crocs: Growing Brand At A Fair Price. It m...\n",
       "1102                                     AEG Over 28.23  \n",
       "5405    Investors are showing more appetite for the ju...\n",
       "                              ...                        \n",
       "599     ed Weekly Triangle on MH,...Net Profit  4,895....\n",
       "5695    Gold Futures Rise To Hover Near Rs 39,800 Amid...\n",
       "1361    Anybody else hearing about one of the large cr...\n",
       "1547    Green Weekly Triangle on PBY,....Open Sell Sho...\n",
       "4959     Weekly.Barely hanging onto double support are...\n",
       "Name: Text, Length: 4053, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       00  000  0025  00pm   01   02  028   03   04   05  ...   œa  \\\n",
       "0     0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "1     0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "2     0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "3     0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "4     0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "...   ...  ...   ...   ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "4048  0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "4049  0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "4050  0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "4051  0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "4052  0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "\n",
       "      œessentialâ  œhysteresisâ   œi  œpandemic  œpriority  œrecalibratingâ  \\\n",
       "0             0.0           0.0  0.0        0.0        0.0              0.0   \n",
       "1             0.0           0.0  0.0        0.0        0.0              0.0   \n",
       "2             0.0           0.0  0.0        0.0        0.0              0.0   \n",
       "3             0.0           0.0  0.0        0.0        0.0              0.0   \n",
       "4             0.0           0.0  0.0        0.0        0.0              0.0   \n",
       "...           ...           ...  ...        ...        ...              ...   \n",
       "4048          0.0           0.0  0.0        0.0        0.0              0.0   \n",
       "4049          0.0           0.0  0.0        0.0        0.0              0.0   \n",
       "4050          0.0           0.0  0.0        0.0        0.0              0.0   \n",
       "4051          0.0           0.0  0.0        0.0        0.0              0.0   \n",
       "4052          0.0           0.0  0.0        0.0        0.0              0.0   \n",
       "\n",
       "      œsince  œwe  œyou  \n",
       "0        0.0  0.0   0.0  \n",
       "1        0.0  0.0   0.0  \n",
       "2        0.0  0.0   0.0  \n",
       "3        0.0  0.0   0.0  \n",
       "4        0.0  0.0   0.0  \n",
       "...      ...  ...   ...  \n",
       "4048     0.0  0.0   0.0  \n",
       "4049     0.0  0.0   0.0  \n",
       "4050     0.0  0.0   0.0  \n",
       "4051     0.0  0.0   0.0  \n",
       "4052     0.0  0.0   0.0  \n",
       "\n",
       "[4053 rows x 8112 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>00</th>\n      <th>000</th>\n      <th>0025</th>\n      <th>00pm</th>\n      <th>01</th>\n      <th>02</th>\n      <th>028</th>\n      <th>03</th>\n      <th>04</th>\n      <th>05</th>\n      <th>...</th>\n      <th>œa</th>\n      <th>œessentialâ</th>\n      <th>œhysteresisâ</th>\n      <th>œi</th>\n      <th>œpandemic</th>\n      <th>œpriority</th>\n      <th>œrecalibratingâ</th>\n      <th>œsince</th>\n      <th>œwe</th>\n      <th>œyou</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4048</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4049</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4050</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4051</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4052</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4053 rows × 8112 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# Convert each entry into a term frequency-inverse document frequency (tfidf) vector \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create the tf-idf feature matrix\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix_test = tfidf.fit_transform(dataframe_comments['comments'].values)\n",
    "tfidf_matrix = tfidf.fit_transform(x_train.values)\n",
    "# show the matrix\n",
    "# tfidf_matrix.toarray()\n",
    "\n",
    "words = tfidf.get_feature_names()\n",
    "\n",
    "tfidf_matrix = pd.DataFrame(tfidf_matrix.toarray(),columns=words)\n",
    "# tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creating a custom Text normalization transformer\n",
    "Text normalization reduces the number of dimensions,\n",
    "decreasing sparsity. Besides the simple filtering of tokens (removing punctuation and\n",
    "stopwords), there are two primary methods for text normalization: stemming and\n",
    "lemmatization.\n",
    "\n",
    "'''\n",
    "import unicodedata\n",
    "import nltk\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, language='english'):\n",
    "        self.stopwords = set(nltk.corpus.stopwords.words(language))\n",
    "        self.lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    def is_punct(self, token):\n",
    "        return all(\n",
    "                    unicodedata.category(char).startswith('P') for char in token\n",
    "                    )\n",
    "\n",
    "    def is_stopword(self, token):\n",
    "        return token.lower() in self.stopwords\n",
    "\n",
    "    def lemmatize(self, token, pos_tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(pos_tag[0], wn.NOUN)\n",
    "        \n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "    def normalize(self, document):\n",
    "        return [\n",
    "            self.lemmatize(token, tag).lower()\n",
    "            for paragraph in document\n",
    "            for sentence in paragraph\n",
    "            for (token, tag) in sentence\n",
    "            if not self.is_punct(token) and not self.is_stopword(token)\n",
    "        ]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        '''\n",
    "        Finally, we must add the Transformer interface, allowing us to add this class to a\n",
    "        Scikit-Learn pipeline, which we’ll explore in the next section:\n",
    "        '''\n",
    "        return self\n",
    "    \n",
    "    def transform(self, documents):\n",
    "         for document in documents:\n",
    "            yield self.normalize(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creating a custom Gensim vectorization transformer\n",
    "Gensim vectorization techniques are an interesting case study because Gensim cor‐\n",
    "pora can be saved and loaded from disk in such a way as to remain decoupled from\n",
    "the pipeline.\n",
    "'''\n",
    "import os\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import sparse2full\n",
    "\n",
    "class GensimVectorizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, path=None):\n",
    "        self.path = path\n",
    "        self.id2word = None\n",
    "        self.load()\n",
    "    \n",
    "    def load(self):\n",
    "        if os.path.exists(self.path):\n",
    "            self.id2word = Dictionary.load(self.path)\n",
    "    \n",
    "    def save(self):\n",
    "        self.id2word.save(self.path)\n",
    "    \n",
    "    def fit(self, documents, labels=None):\n",
    "        self.id2word = Dictionary(documents)\n",
    "        self.save()\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        for document in documents:\n",
    "            docvec = self.id2word.doc2bow(document)\n",
    "            yield sparse2full(docvec, len(self.id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "'''\n",
    "Scikit-Learn's Pipeline objects enable us to integrate a series of transformers that combine normal‐\n",
    "ization, vectorization, and feature analysis into a single, well-defined mechanism.The purpose of a Pipeline is to chain together multiple estimators representing a fixed sequence of steps into a single unit. All estimators in the pipeline, except the last one, must be transformers. Pipelines are constructed by describing a list of (key, value) pairs where the key is a string that names the step and the value is the estimator object.\n",
    "'''\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = Pipeline([\n",
    "    ('normalizer', TextNormalizer()),\n",
    "    ('vectorizer', GensimVectorizer),\n",
    "    ('bayes', MultinomialNB()),\n",
    "])\n",
    "a = y_train\n",
    "# model.named_steps['bayes']\n",
    "# model.steps[2]\n",
    "# model(tfidf_matrix)\n",
    "# model.named_steps['bayes'].fit(tfidf_matrix,a).predict(tfidf_matrix)\n",
    "model.named_steps['bayes'].fit(tfidf_matrix,a)\n",
    "model.named_steps['bayes'].predict()\n",
    "\n",
    "# model.named_steps['normalizer'].transform(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   are       bad  birthday       day      good  happening    having  hello  \\\n",
       "0  0.5  0.000000    0.0000  0.000000  0.000000   0.000000  0.000000    0.5   \n",
       "1  0.0  0.000000    0.0000  0.416629  0.547817   0.000000  0.000000    0.0   \n",
       "2  0.0  0.000000    0.3337  0.180070  0.000000   0.000000  0.000000    0.0   \n",
       "3  0.0  0.362620    0.0000  0.471459  0.309956   0.000000  0.000000    0.0   \n",
       "4  0.0  0.589882    0.0000  0.383466  0.000000   0.000000  0.710628    0.0   \n",
       "5  0.0  0.000000    0.0000  0.000000  0.000000   0.495772  0.000000    0.0   \n",
       "6  0.0  0.000000    0.0000  0.329775  0.433614   0.000000  0.000000    0.0   \n",
       "\n",
       "   how        is  ...   neither   nothing        or  really      so   special  \\\n",
       "0  0.5  0.000000  ...  0.000000  0.000000  0.000000  0.0000  0.0000  0.000000   \n",
       "1  0.0  0.547817  ...  0.000000  0.000000  0.000000  0.0000  0.0000  0.000000   \n",
       "2  0.0  0.000000  ...  0.000000  0.000000  0.000000  0.3337  0.3337  0.277000   \n",
       "3  0.0  0.309956  ...  0.436847  0.000000  0.436847  0.0000  0.0000  0.000000   \n",
       "4  0.0  0.000000  ...  0.000000  0.000000  0.000000  0.0000  0.0000  0.000000   \n",
       "5  0.0  0.000000  ...  0.000000  0.495772  0.000000  0.0000  0.0000  0.411533   \n",
       "6  0.0  0.433614  ...  0.000000  0.000000  0.000000  0.0000  0.0000  0.000000   \n",
       "\n",
       "      super     there     today  you  \n",
       "0  0.000000  0.000000  0.000000  0.5  \n",
       "1  0.000000  0.000000  0.475619  0.0  \n",
       "2  0.000000  0.000000  0.000000  0.0  \n",
       "3  0.000000  0.000000  0.269106  0.0  \n",
       "4  0.000000  0.000000  0.000000  0.0  \n",
       "5  0.000000  0.495772  0.305405  0.0  \n",
       "6  0.611129  0.000000  0.376467  0.0  \n",
       "\n",
       "[7 rows x 22 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>are</th>\n      <th>bad</th>\n      <th>birthday</th>\n      <th>day</th>\n      <th>good</th>\n      <th>happening</th>\n      <th>having</th>\n      <th>hello</th>\n      <th>how</th>\n      <th>is</th>\n      <th>...</th>\n      <th>neither</th>\n      <th>nothing</th>\n      <th>or</th>\n      <th>really</th>\n      <th>so</th>\n      <th>special</th>\n      <th>super</th>\n      <th>there</th>\n      <th>today</th>\n      <th>you</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.5</td>\n      <td>0.000000</td>\n      <td>0.0000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0000</td>\n      <td>0.416629</td>\n      <td>0.547817</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.547817</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.475619</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.3337</td>\n      <td>0.180070</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.3337</td>\n      <td>0.3337</td>\n      <td>0.277000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.362620</td>\n      <td>0.0000</td>\n      <td>0.471459</td>\n      <td>0.309956</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.309956</td>\n      <td>...</td>\n      <td>0.436847</td>\n      <td>0.000000</td>\n      <td>0.436847</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.269106</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.589882</td>\n      <td>0.0000</td>\n      <td>0.383466</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.710628</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.495772</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.495772</td>\n      <td>0.000000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.411533</td>\n      <td>0.000000</td>\n      <td>0.495772</td>\n      <td>0.305405</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0000</td>\n      <td>0.329775</td>\n      <td>0.433614</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.433614</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.000000</td>\n      <td>0.611129</td>\n      <td>0.000000</td>\n      <td>0.376467</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>7 rows × 22 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  }
 ]
}