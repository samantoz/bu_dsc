{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json library to read data in jsonl file\n",
    "import json\n",
    "#import pandas library\n",
    "import pandas as pd\n",
    "#import numpy library\n",
    "import numpy as np\n",
    "#import regular expressions library\n",
    "import re\n",
    "#import nltk\n",
    "import nltk\n",
    "#import stopwords from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "#import word tokenizer from NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "#import Part-of-Speech tagger\n",
    "from nltk import pos_tag\n",
    "#import sklearn\n",
    "import sklearn\n",
    "#import word count vectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#import tf-idf vectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#import the one-hot encoding package from sklearn\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version: 1.2.4\n",
      "numpy version: 1.19.2\n",
      "scikit-learn version: 0.23.2\n",
      "NLTK version: 3.5\n"
     ]
    }
   ],
   "source": [
    "#check versions of packages\n",
    "print('pandas version:', pd.__version__)\n",
    "print('numpy version:', np.__version__)\n",
    "print('scikit-learn version:', sklearn.__version__)\n",
    "print('NLTK version:', nltk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>con</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Well it's great that he did something about th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You are right Mr. President.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>You have given no input apart from saying I am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>I get the frustration but the reason they want...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I am far from an expert on TPP and I would ten...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   con                                                txt\n",
       "0    0  Well it's great that he did something about th...\n",
       "1    0                       You are right Mr. President.\n",
       "2    0  You have given no input apart from saying I am...\n",
       "3    0  I get the frustration but the reason they want...\n",
       "4    0  I am far from an expert on TPP and I would ten..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in the data as a dataframe\n",
    "filename = \"~/Documents/mygithub/bu_dsc/data/raw/controversial-comments.jsonl\"\n",
    "df_all = pd.read_json(filename, lines = True)\n",
    "#display the first few rows of data\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parts A and B: Convert text to lowercase and romove punctuation\n",
    "#define a function to clean the text\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Remove punctuations and special characters, makes lower case\n",
    "    Args: text\n",
    "    Output: text\n",
    "    \"\"\"\n",
    "    text=text.lower() #makes text lowercase\n",
    "    text=re.sub('\\\\d|\\\\W+|_',' ',text) #removes extra white space\n",
    "    text=re.sub('[^a-zA-Z]',\" \", text) #removes any non-alphabetic characters\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part C: Remove stop words (and tokenize)\n",
    "#define a function to tokenize the text and remove stop words\n",
    "#use the nltk package for tokenizing and removing stop words\n",
    "#Note: You may have to run this next commmand to download the NLTK 'punkt' library for the first time\n",
    "#nltk.download('punkt')\n",
    "#Note: You may need to run this next command to download stopwords for the first time\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "def tokenize_and_remove_stop_words(txt):\n",
    "    \"\"\"\n",
    "    takes in a sentence, tokenizes the words into a list,\n",
    "    and then removes stop words from the tokenized list\n",
    "    \"\"\"\n",
    "    stop_words = stopwords.words('english')\n",
    "    txt_token = word_tokenize(txt)\n",
    "    txt_no_stopwords = [word for word in txt_token if word not in stop_words]\n",
    "    return txt_no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part D: Apply NLTK's PorterStemmer\n",
    "#define a function to stem the words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def stem_text(word_list):\n",
    "\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(word) for word in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>con</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[deleted]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Vlad has more connects in the White Pride Hous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Lol there would never be a liberal president e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>You should care about the Bible because the Ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>This submission has been automatically removed...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   con                                                txt\n",
       "0    0                                          [deleted]\n",
       "1    0  Vlad has more connects in the White Pride Hous...\n",
       "2    0  Lol there would never be a liberal president e...\n",
       "3    0  You should care about the Bible because the Ki...\n",
       "4    0  This submission has been automatically removed..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take a random sample of the dataframe to cut down on processing time\n",
    "#number of comments to keep\n",
    "num_comments = 50000\n",
    "df_sample = df_all.sample(n = num_comments).reset_index(drop = True )\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>con</th>\n",
       "      <th>txt</th>\n",
       "      <th>txt_clean</th>\n",
       "      <th>txt_tokenized</th>\n",
       "      <th>txt_stemmed</th>\n",
       "      <th>txt_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>deleted</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>[delet]</td>\n",
       "      <td>delet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Vlad has more connects in the White Pride Hous...</td>\n",
       "      <td>vlad has more connects in the white pride hous...</td>\n",
       "      <td>[vlad, connects, white, pride, house, flynn, f...</td>\n",
       "      <td>[vlad, connect, white, pride, hous, flynn, fly...</td>\n",
       "      <td>vlad connect white pride hous flynn flynn one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Lol there would never be a liberal president e...</td>\n",
       "      <td>lol there would never be a liberal president e...</td>\n",
       "      <td>[lol, would, never, liberal, president, ever]</td>\n",
       "      <td>[lol, would, never, liber, presid, ever]</td>\n",
       "      <td>lol would never liber presid ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>You should care about the Bible because the Ki...</td>\n",
       "      <td>you should care about the bible because the ki...</td>\n",
       "      <td>[care, bible, king, james, version, work, art,...</td>\n",
       "      <td>[care, bibl, king, jame, version, work, art, s...</td>\n",
       "      <td>care bibl king jame version work art sourc man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>This submission has been automatically removed...</td>\n",
       "      <td>this submission has been automatically removed...</td>\n",
       "      <td>[submission, automatically, removed, either, l...</td>\n",
       "      <td>[submiss, automat, remov, either, link, shorte...</td>\n",
       "      <td>submiss automat remov either link shorten link...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   con                                                txt  \\\n",
       "0    0                                          [deleted]   \n",
       "1    0  Vlad has more connects in the White Pride Hous...   \n",
       "2    0  Lol there would never be a liberal president e...   \n",
       "3    0  You should care about the Bible because the Ki...   \n",
       "4    0  This submission has been automatically removed...   \n",
       "\n",
       "                                           txt_clean  \\\n",
       "0                                           deleted    \n",
       "1  vlad has more connects in the white pride hous...   \n",
       "2  lol there would never be a liberal president e...   \n",
       "3  you should care about the bible because the ki...   \n",
       "4  this submission has been automatically removed...   \n",
       "\n",
       "                                       txt_tokenized  \\\n",
       "0                                          [deleted]   \n",
       "1  [vlad, connects, white, pride, house, flynn, f...   \n",
       "2      [lol, would, never, liberal, president, ever]   \n",
       "3  [care, bible, king, james, version, work, art,...   \n",
       "4  [submission, automatically, removed, either, l...   \n",
       "\n",
       "                                         txt_stemmed  \\\n",
       "0                                            [delet]   \n",
       "1  [vlad, connect, white, pride, hous, flynn, fly...   \n",
       "2           [lol, would, never, liber, presid, ever]   \n",
       "3  [care, bibl, king, jame, version, work, art, s...   \n",
       "4  [submiss, automat, remov, either, link, shorte...   \n",
       "\n",
       "                                           txt_final  \n",
       "0                                              delet  \n",
       "1  vlad connect white pride hous flynn flynn one ...  \n",
       "2                  lol would never liber presid ever  \n",
       "3  care bibl king jame version work art sourc man...  \n",
       "4  submiss automat remov either link shorten link...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a new data frame for the column for each pre-processing step\n",
    "#apply text cleaning function\n",
    "df_sample['txt_clean'] = df_sample['txt'].apply(clean_text)\n",
    "#apply tokenizing/removing stop words function\n",
    "df_sample['txt_tokenized'] = df_sample['txt_clean'].apply(tokenize_and_remove_stop_words)\n",
    "#apply PorterStemmer function\n",
    "df_sample['txt_stemmed'] = df_sample['txt_tokenized'].apply(stem_text)\n",
    "#put the text back together (untokenize)\n",
    "df_sample['txt_final'] = df_sample['txt_stemmed'].apply(lambda text: ' '.join(text))\n",
    "#view the pre-processed text\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the dimensions of the dataframe\n",
    "df_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the dataframe has 50,000 rows (comments). Because we want to make a prediction about each comment,our input for modeling should also have 50,000 rows. A good way to check to make sure the following steps are working properly is by checking the dimensions of the ouput array, and we know it should have 50,000 rows. If there are not 50,000 rows, something was not done correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the Word Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the bag of words feature matrix\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(df_sample['txt_final'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 24144)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the shape of the output\n",
    "bag_of_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that there are 50,000 rows as expected, and the 24,080 columns correspond to the unique words in the comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the TFIDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to get the tf-idf vectorization\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(df_sample['txt_final'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 24144)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the shape of the output\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the TFIDF matrix has the same shape as the word count vector. This makes sense as the number of columns in both matrices corresponds to the\n",
    "number of unique words. Whereas the word count vectorizer is only counting the number of times the word appears the TFIDF vectorization weighs how\n",
    "important each word is in each comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the part-of-speech tagging\n",
    "#note we are applying this to the tokenized comments\n",
    "pos_matrix = df_sample['txt_tokenized'].apply(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vlad connect white pride hous flynn flynn one took order well brought back ripe inform secur council vlad could start kill spi immedi vlad still bannon tillerson cohen manafort live trump tower nyc right close nyc fbi offic carter page stone none secur clearanc job flynn donni addl brain pass reliabl inform onto folk mean vlad well\n",
      "[('vlad', 'JJ'), ('connects', 'NNS'), ('white', 'JJ'), ('pride', 'NN'), ('house', 'NN'), ('flynn', 'VBD'), ('flynn', 'JJ'), ('one', 'CD'), ('took', 'VBD'), ('orders', 'NNS'), ('well', 'RB'), ('brought', 'VBD'), ('back', 'RP'), ('ripe', 'JJ'), ('information', 'NN'), ('security', 'NN'), ('council', 'NN'), ('vlad', 'NN'), ('could', 'MD'), ('start', 'VB'), ('killing', 'VBG'), ('spies', 'NNS'), ('immediately', 'RB'), ('vlad', 'VBP'), ('still', 'RB'), ('bannon', 'VBN'), ('tillerson', 'NN'), ('cohen', 'NN'), ('manafort', 'NN'), ('living', 'VBG'), ('trump', 'NN'), ('tower', 'NN'), ('nyc', 'RB'), ('right', 'RB'), ('close', 'JJ'), ('nyc', 'NNS'), ('fbi', 'JJ'), ('office', 'NN'), ('carter', 'NN'), ('page', 'NN'), ('stone', 'NN'), ('none', 'NN'), ('security', 'NN'), ('clearance', 'NN'), ('jobs', 'NNS'), ('flynn', 'VBP'), ('donny', 'NN'), ('addled', 'VBD'), ('brain', 'NN'), ('pass', 'NN'), ('reliable', 'JJ'), ('information', 'NN'), ('onto', 'IN'), ('folks', 'NNS'), ('means', 'NNS'), ('vlad', 'VBP'), ('well', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "#print the first non stemmed/tokenized comment\n",
    "print(df_sample['txt_final'][1])\n",
    "#show the first entry of the part-of-speech matrix\n",
    "print(pos_matrix[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it takes in the tokenized comment and attaches a part-of-speech to it. E.g., 'NN' is a noun and 'JJ' is an adjective. There was some discussion of\n",
    "whether the part-of-speech tagging should be applied to the non-stemmed text as stemming can change the part-of-speech. So I did the POS tagging to the non stemmed words.But keep this in mind.\n",
    "\n",
    "This matrix is not numerical and thus cannot be used as input to a model. To complete the process, we will apply one-hot encoding to this matrix to be used for input to a model. Again, the number of rows in this matrix should still be 50,000 but the number of columns will correspond to each different part-of-speech in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the tags only for the one-hot-encoding\n",
    "tags = []\n",
    "for pos_tag in pos_matrix:\n",
    "    tags.append([tag for word, tag in pos_tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words and Parts-of-Speech: [('vlad', 'JJ'), ('connects', 'NNS'), ('white', 'JJ'), ('pride', 'NN'), ('house', 'NN'), ('flynn', 'VBD'), ('flynn', 'JJ'), ('one', 'CD'), ('took', 'VBD'), ('orders', 'NNS'), ('well', 'RB'), ('brought', 'VBD'), ('back', 'RP'), ('ripe', 'JJ'), ('information', 'NN'), ('security', 'NN'), ('council', 'NN'), ('vlad', 'NN'), ('could', 'MD'), ('start', 'VB'), ('killing', 'VBG'), ('spies', 'NNS'), ('immediately', 'RB'), ('vlad', 'VBP'), ('still', 'RB'), ('bannon', 'VBN'), ('tillerson', 'NN'), ('cohen', 'NN'), ('manafort', 'NN'), ('living', 'VBG'), ('trump', 'NN'), ('tower', 'NN'), ('nyc', 'RB'), ('right', 'RB'), ('close', 'JJ'), ('nyc', 'NNS'), ('fbi', 'JJ'), ('office', 'NN'), ('carter', 'NN'), ('page', 'NN'), ('stone', 'NN'), ('none', 'NN'), ('security', 'NN'), ('clearance', 'NN'), ('jobs', 'NNS'), ('flynn', 'VBP'), ('donny', 'NN'), ('addled', 'VBD'), ('brain', 'NN'), ('pass', 'NN'), ('reliable', 'JJ'), ('information', 'NN'), ('onto', 'IN'), ('folks', 'NNS'), ('means', 'NNS'), ('vlad', 'VBP'), ('well', 'RB')]\n",
      "Parts-of-Speech Only: ['JJ', 'NNS', 'JJ', 'NN', 'NN', 'VBD', 'JJ', 'CD', 'VBD', 'NNS', 'RB', 'VBD', 'RP', 'JJ', 'NN', 'NN', 'NN', 'NN', 'MD', 'VB', 'VBG', 'NNS', 'RB', 'VBP', 'RB', 'VBN', 'NN', 'NN', 'NN', 'VBG', 'NN', 'NN', 'RB', 'RB', 'JJ', 'NNS', 'JJ', 'NN', 'NN', 'NN', 'NN', 'NN', 'NN', 'NN', 'NNS', 'VBP', 'NN', 'VBD', 'NN', 'NN', 'JJ', 'NN', 'IN', 'NNS', 'NNS', 'VBP', 'RB']\n"
     ]
    }
   ],
   "source": [
    "#Let's display what this did\n",
    "#print the first entry in the part-of-speech matrix\n",
    "print('Words and Parts-of-Speech:', pos_matrix[1])\n",
    "#print the first entry in tags\n",
    "print('Parts-of-Speech Only:', tags[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the one-hot encoder\n",
    "one_hot_multi = MultiLabelBinarizer()\n",
    "#one-hot encode the pos tags\n",
    "pos_num_matrix = one_hot_multi.fit_transform(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 37)\n",
      "['$' \"''\" 'CC' 'CD' 'DT' 'EX' 'FW' 'IN' 'JJ' 'JJR' 'JJS' 'MD' 'NN' 'NNP'\n",
      " 'NNPS' 'NNS' 'PDT' 'POS' 'PRP' 'PRP$' 'RB' 'RBR' 'RBS' 'RP' 'SYM' 'TO'\n",
      " 'UH' 'VB' 'VBD' 'VBG' 'VBN' 'VBP' 'VBZ' 'WDT' 'WP' 'WP$' 'WRB']\n"
     ]
    }
   ],
   "source": [
    "#Let's looks at the shape and classes of the output matrix\n",
    "print(pos_num_matrix.shape)\n",
    "print(one_hot_multi.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, as expected, we have 50,000 rows. There are 37 columns corresponding the different parts-of-speech appearing in the comments. The classes shown correspond to each of the columns in the part-of-speech numerical matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python38564bitbaseconda9c4f9ef913ec46fba5f8aa62969238f5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
