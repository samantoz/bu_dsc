{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json library to read data in jsonl file\n",
    "import json\n",
    "#import pandas library\n",
    "import pandas as pd\n",
    "#import numpy library\n",
    "import numpy as np\n",
    "#import regular expressions library\n",
    "import re\n",
    "#import nltk\n",
    "import nltk\n",
    "#import stopwords from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "#import word tokenizer from NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "#import Part-of-Speech tagger\n",
    "from nltk import pos_tag\n",
    "#import sklearn\n",
    "import sklearn\n",
    "#import word count vectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#import tf-idf vectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#import the one-hot encoding package from sklearn\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pandas version: 1.2.4\nnumpy version: 1.19.2\nscikit-learn version: 0.23.2\nNLTK version: 3.5\n"
     ]
    }
   ],
   "source": [
    "#check versions of packages\n",
    "print('pandas version:', pd.__version__)\n",
    "print('numpy version:', np.__version__)\n",
    "print('scikit-learn version:', sklearn.__version__)\n",
    "print('NLTK version:', nltk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   con                                                txt\n",
       "0    0  Well it's great that he did something about th...\n",
       "1    0                       You are right Mr. President.\n",
       "2    0  You have given no input apart from saying I am...\n",
       "3    0  I get the frustration but the reason they want...\n",
       "4    0  I am far from an expert on TPP and I would ten..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>con</th>\n      <th>txt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Well it's great that he did something about th...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>You are right Mr. President.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>You have given no input apart from saying I am...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>I get the frustration but the reason they want...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>I am far from an expert on TPP and I would ten...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "#read in the data as a dataframe\n",
    "filename = \"~/Documents/mygithub/bu_dsc/data/raw/controversial-comments.jsonl\"\n",
    "df_all = pd.read_json(filename, lines = True)\n",
    "#display the first few rows of data\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parts A and B: Convert text to lowercase and romove punctuation\n",
    "#define a function to clean the text\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Remove punctuations and special characters, makes lower case\n",
    "    Args: text\n",
    "    Output: text\n",
    "    \"\"\"\n",
    "    text=text.lower() #makes text lowercase\n",
    "    text=re.sub('\\\\d|\\\\W+|_',' ',text) #removes extra white space\n",
    "    text=re.sub('[^a-zA-Z]',\" \", text) #removes any non-alphabetic characters\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part C: Remove stop words (and tokenize)\n",
    "#define a function to tokenize the text and remove stop words\n",
    "#use the nltk package for tokenizing and removing stop words\n",
    "#Note: You may have to run this next commmand to download the NLTK 'punkt' library for the first time\n",
    "#nltk.download('punkt')\n",
    "#Note: You may need to run this next command to download stopwords for the first time\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "def tokenize_and_remove_stop_words(txt):\n",
    "    \"\"\"\n",
    "    takes in a sentence, tokenizes the words into a list,\n",
    "    and then removes stop words from the tokenized list\n",
    "    \"\"\"\n",
    "    stop_words = stopwords.words('english')\n",
    "    txt_token = word_tokenize(txt)\n",
    "    txt_no_stopwords = [word for word in txt_token if word not in stop_words]\n",
    "    return txt_no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part D: Apply NLTK's PorterStemmer\n",
    "#define a function to stem the words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def stem_text(word_list):\n",
    "\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(word) for word in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   con                                                txt\n",
       "0    0  Nothing or Anyone Bill Clinton has done, is do...\n",
       "1    0                                          [deleted]\n",
       "2    0                                         Consarnit!\n",
       "3    0  Not only bringing balance to the force but bri...\n",
       "4    0  I see you're a frequent poster on t_d. Lovely ..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>con</th>\n      <th>txt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Nothing or Anyone Bill Clinton has done, is do...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>[deleted]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>Consarnit!</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>Not only bringing balance to the force but bri...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>I see you're a frequent poster on t_d. Lovely ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "#take a random sample of the dataframe to cut down on processing time\n",
    "#number of comments to keep\n",
    "num_comments = 50000\n",
    "df_sample = df_all.sample(n = num_comments).reset_index(drop = True )\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   con                                                txt  \\\n",
       "0    0  Nothing or Anyone Bill Clinton has done, is do...   \n",
       "1    0                                          [deleted]   \n",
       "2    0                                         Consarnit!   \n",
       "3    0  Not only bringing balance to the force but bri...   \n",
       "4    0  I see you're a frequent poster on t_d. Lovely ...   \n",
       "\n",
       "                                           txt_clean  \\\n",
       "0  nothing or anyone bill clinton has done is doi...   \n",
       "1                                           deleted    \n",
       "2                                         consarnit    \n",
       "3  not only bringing balance to the force but bri...   \n",
       "4  i see you re a frequent poster on t d lovely p...   \n",
       "\n",
       "                                       txt_tokenized  \\\n",
       "0  [nothing, anyone, bill, clinton, done, bearing...   \n",
       "1                                          [deleted]   \n",
       "2                                        [consarnit]   \n",
       "3  [bringing, balance, force, bringing, peace, de...   \n",
       "4            [see, frequent, poster, lovely, person]   \n",
       "\n",
       "                                         txt_stemmed  \\\n",
       "0  [noth, anyon, bill, clinton, done, bear, want,...   \n",
       "1                                            [delet]   \n",
       "2                                        [consarnit]   \n",
       "3  [bring, balanc, forc, bring, peac, decad, old,...   \n",
       "4              [see, frequent, poster, love, person]   \n",
       "\n",
       "                                           txt_final  \n",
       "0  noth anyon bill clinton done bear want shape n...  \n",
       "1                                              delet  \n",
       "2                                          consarnit  \n",
       "3  bring balanc forc bring peac decad old feud or...  \n",
       "4                    see frequent poster love person  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>con</th>\n      <th>txt</th>\n      <th>txt_clean</th>\n      <th>txt_tokenized</th>\n      <th>txt_stemmed</th>\n      <th>txt_final</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Nothing or Anyone Bill Clinton has done, is do...</td>\n      <td>nothing or anyone bill clinton has done is doi...</td>\n      <td>[nothing, anyone, bill, clinton, done, bearing...</td>\n      <td>[noth, anyon, bill, clinton, done, bear, want,...</td>\n      <td>noth anyon bill clinton done bear want shape n...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>[deleted]</td>\n      <td>deleted</td>\n      <td>[deleted]</td>\n      <td>[delet]</td>\n      <td>delet</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>Consarnit!</td>\n      <td>consarnit</td>\n      <td>[consarnit]</td>\n      <td>[consarnit]</td>\n      <td>consarnit</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>Not only bringing balance to the force but bri...</td>\n      <td>not only bringing balance to the force but bri...</td>\n      <td>[bringing, balance, force, bringing, peace, de...</td>\n      <td>[bring, balanc, forc, bring, peac, decad, old,...</td>\n      <td>bring balanc forc bring peac decad old feud or...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>I see you're a frequent poster on t_d. Lovely ...</td>\n      <td>i see you re a frequent poster on t d lovely p...</td>\n      <td>[see, frequent, poster, lovely, person]</td>\n      <td>[see, frequent, poster, love, person]</td>\n      <td>see frequent poster love person</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "#create a new data frame for the column for each pre-processing step\n",
    "#apply text cleaning function\n",
    "df_sample['txt_clean'] = df_sample['txt'].apply(clean_text)\n",
    "#apply tokenizing/removing stop words function\n",
    "df_sample['txt_tokenized'] = df_sample['txt_clean'].apply(tokenize_and_remove_stop_words)\n",
    "#apply PorterStemmer function\n",
    "df_sample['txt_stemmed'] = df_sample['txt_tokenized'].apply(stem_text)\n",
    "#put the text back together (untokenize)\n",
    "df_sample['txt_final'] = df_sample['txt_stemmed'].apply(lambda text: ' '.join(text))\n",
    "#view the pre-processed text\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(50000, 6)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "#check the dimensions of the dataframe\n",
    "df_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the dataframe has 50,000 rows (comments). Because we want to make a prediction about each comment,our input for modeling should also have 50,000 rows. A good way to check to make sure the following steps are working properly is by checking the dimensions of the ouput array, and we know it should have 50,000 rows. If there are not 50,000 rows, something was not done correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the Word Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the bag of words feature matrix\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(df_sample['txt_final'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(50000, 24354)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "#check the shape of the output\n",
    "bag_of_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that there are 50,000 rows as expected, and the 24,080 columns correspond to the unique words in the comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the TFIDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to get the tf-idf vectorization\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(df_sample['txt_final'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(50000, 24354)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "#check the shape of the output\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the TFIDF matrix has the same shape as the word count vector. This makes sense as the number of columns in both matrices corresponds to the\n",
    "number of unique words. Whereas the word count vectorizer is only counting the number of times the word appears the TFIDF vectorization weighs how\n",
    "important each word is in each comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the part-of-speech tagging\n",
    "#note we are applying this to the tokenized comments\n",
    "pos_matrix = df_sample['txt_tokenized'].apply(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "delet\n[('deleted', 'VBN')]\n"
     ]
    }
   ],
   "source": [
    "#print the first non stemmed/tokenized comment\n",
    "print(df_sample['txt_final'][1])\n",
    "#show the first entry of the part-of-speech matrix\n",
    "print(pos_matrix[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it takes in the tokenized comment and attaches a part-of-speech to it. E.g., 'NN' is a noun and 'JJ' is an adjective. There was some discussion of\n",
    "whether the part-of-speech tagging should be applied to the non-stemmed text as stemming can change the part-of-speech. So I did the POS tagging to the non stemmed words.But keep this in mind.\n",
    "\n",
    "This matrix is not numerical and thus cannot be used as input to a model. To complete the process, we will apply one-hot encoding to this matrix to be used for input to a model. Again, the number of rows in this matrix should still be 50,000 but the number of columns will correspond to each different part-of-speech in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the tags only for the one-hot-encoding\n",
    "tags = []\n",
    "for pos_tag in pos_matrix:\n",
    "    tags.append([tag for word, tag in pos_tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Words and Parts-of-Speech: [('deleted', 'VBN')]\nParts-of-Speech Only: ['VBN']\n"
     ]
    }
   ],
   "source": [
    "#Let's display what this did\n",
    "#print the first entry in the part-of-speech matrix\n",
    "print('Words and Parts-of-Speech:', pos_matrix[1])\n",
    "#print the first entry in tags\n",
    "print('Parts-of-Speech Only:', tags[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the one-hot encoder\n",
    "one_hot_multi = MultiLabelBinarizer()\n",
    "#one-hot encode the pos tags\n",
    "pos_num_matrix = one_hot_multi.fit_transform(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(50000, 36)\n[\"''\" 'CC' 'CD' 'DT' 'EX' 'FW' 'IN' 'JJ' 'JJR' 'JJS' 'MD' 'NN' 'NNP'\n 'NNPS' 'NNS' 'PDT' 'POS' 'PRP' 'PRP$' 'RB' 'RBR' 'RBS' 'RP' 'SYM' 'TO'\n 'UH' 'VB' 'VBD' 'VBG' 'VBN' 'VBP' 'VBZ' 'WDT' 'WP' 'WP$' 'WRB']\n"
     ]
    }
   ],
   "source": [
    "#Let's looks at the shape and classes of the output matrix\n",
    "print(pos_num_matrix.shape)\n",
    "print(one_hot_multi.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, as expected, we have 50,000 rows. There are 37 columns corresponding the different parts-of-speech appearing in the comments. The classes shown correspond to each of the columns in the part-of-speech numerical matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python38564bitbaseconda9c4f9ef913ec46fba5f8aa62969238f5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}