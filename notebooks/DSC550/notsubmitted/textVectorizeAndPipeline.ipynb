{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitbaseconda9c4f9ef913ec46fba5f8aa62969238f5",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "The tokenize method performs some lightweight normalization,stripping punctuation and setting the text to lower case.\n",
    "'''\n",
    "\n",
    "def tokenize(text):\n",
    "    stem = nltk.stem.SnowballStemmer('english')\n",
    "    text = text.lower()\n",
    "\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        if token in string.punctuation: continue\n",
    "        yield stem.stem(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def vectorize(doc):\n",
    "    features = defaultdict(int)\n",
    "    for token in tokenize(doc):\n",
    "        features[token] += 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus =[\n",
    "    \"The elephant sneezed at the sight of potatoes.\",\n",
    "    \"Bats can see via echolocation. See the bat sight sneeze!\",\n",
    "    \"Wondering, she opened the door to the studio.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   0\n",
       "0     The elephant sneezed at the sight of potatoes.\n",
       "1  Bats can see via echolocation. See the bat sig...\n",
       "2      Wondering, she opened the door to the studio."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The elephant sneezed at the sight of potatoes.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Bats can see via echolocation. See the bat sig...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Wondering, she opened the door to the studio.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df = pd.DataFrame(corpus)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[defaultdict(<class 'int'>, {'the': 2, 'eleph': 1, 'sneez': 1, 'at': 1, 'sight': 1, 'of': 1, 'potato': 1}), defaultdict(<class 'int'>, {'bat': 2, 'can': 1, 'see': 2, 'via': 1, 'echoloc': 1, 'the': 1, 'sight': 1, 'sneez': 1}), defaultdict(<class 'int'>, {'wonder': 1, 'she': 1, 'open': 1, 'the': 2, 'door': 1, 'to': 1, 'studio': 1})]\n"
     ]
    }
   ],
   "source": [
    "vectors_nltk = map(vectorize,corpus)\n",
    "print(list(vectors_nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectors_sk = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['at', 'bat', 'bats', 'can', 'door', 'echolocation', 'elephant', 'of', 'opened', 'potatoes', 'see', 'she', 'sight', 'sneeze', 'sneezed', 'studio', 'the', 'to', 'via', 'wondering']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 2 0 0 0]\n [0 1 1 1 0 1 0 0 0 0 2 0 1 1 0 0 1 0 1 0]\n [0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 2 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(vectors_sk.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "'''\n",
    "The Estimator.fit method sets the state of the estimator based on the training data,\n",
    "X and y. The training data X is expected to be matrix-like—for example, a two-\n",
    "dimensional NumPy array of shape (n_samples, n_features).\n",
    "y is the supervised estimator which is fit with a one-dimensional Numpy array.\n",
    "'''\n",
    "\n",
    "class Estimator(BaseEstimator):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        '''\n",
    "        Accept input data, X, and optional target data, y. Returns self.\n",
    "        '''\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Accept input data, X and return a vector of predictions for each row.\n",
    "        '''\n",
    "        return yhat\n",
    "\n",
    "'''\n",
    "A Transformer is a special type of Estimator that creates a new dataset\n",
    "from an old one based on rules that it has learned from the fitting process.\n",
    "'''\n",
    "\n",
    "class Transformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        '''\n",
    "        Learn how to transform data based on input data, X.\n",
    "        '''\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform X into a new dataset, Xprime and return it.\n",
    "        \"\"\"\n",
    "        return Xprime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creating a custom Gensim vectorization transformer\n",
    "Gensim vectorization techniques are an interesting case study because Gensim cor‐\n",
    "pora can be saved and loaded from disk in such a way as to remain decoupled from\n",
    "the pipeline.\n",
    "'''\n",
    "import os\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import sparse2full\n",
    "\n",
    "class GensimVectorizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, path=None):\n",
    "        self.path = path\n",
    "        self.id2word = None\n",
    "        self.load()\n",
    "    \n",
    "    def load(self):\n",
    "        if os.path.exists(self.path):\n",
    "            self.id2word = Dictionary.load(self.path)\n",
    "    \n",
    "    def save(self):\n",
    "        self.id2word.save(self.path)\n",
    "    \n",
    "    def fit(self, documents, labels=None):\n",
    "        self.id2word = Dictionary(documents)\n",
    "        self.save()\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        for document in documents:\n",
    "            docvec = self.id2word.doc2bow(document)\n",
    "            yield sparse2full(docvec, len(self.id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creating a custom Text normalization transformer\n",
    "Text normalization reduces the number of dimensions,\n",
    "decreasing sparsity. Besides the simple filtering of tokens (removing punctuation and\n",
    "stopwords), there are two primary methods for text normalization: stemming and\n",
    "lemmatization.\n",
    "\n",
    "'''\n",
    "import unicodedata\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, language='english'):\n",
    "        self.stopwords = set(nltk.corpus.stopwords.words(language))\n",
    "        self.lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    def is_punct(self, token):\n",
    "        return all(\n",
    "                    unicodedata.category(char).startswith('P') for char in token\n",
    "                    )\n",
    "\n",
    "    def is_stopword(self, token):\n",
    "        return token.lower() in self.stopwords\n",
    "\n",
    "    def lemmatize(self, token, pos_tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(pos_tag[0], wn.NOUN)\n",
    "        \n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "    def normalize(self, document):\n",
    "        return [\n",
    "            self.lemmatize(token, tag).lower()\n",
    "            for paragraph in document\n",
    "            for sentence in paragraph\n",
    "            for (token, tag) in sentence\n",
    "            if not self.is_punct(token) and not self.is_stopword(token)\n",
    "        ]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        '''\n",
    "        Finally, we must add the Transformer interface, allowing us to add this class to a\n",
    "        Scikit-Learn pipeline, which we’ll explore in the next section:\n",
    "        '''\n",
    "        return self\n",
    "    \n",
    "    def transform(self, documents):\n",
    "         for document in documents:\n",
    "            yield self.normalize(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TextNormalizer(language=None)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "'''\n",
    "Scikit-Learn's Pipeline objects enable us to integrate a series of transformers that combine normal‐\n",
    "ization, vectorization, and feature analysis into a single, well-defined mechanism.The purpose of a Pipeline is to chain together multiple estimators representing a fixed sequence of steps into a single unit. All estimators in the pipeline, except the last one, must be transformers. Pipelines are constructed by describing a list of (key, value) pairs where the key is a string that names the step and the value is the estimator object.\n",
    "'''\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = Pipeline([\n",
    "    ('normalizer', TextNormalizer()),\n",
    "    ('vectorizer', GensimVectorizer),\n",
    "    ('bayes', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# model.named_steps['bayes']\n",
    "# model.steps[2]\n",
    "\n",
    "model.named_steps['normalizer'].fit(df)\n",
    "# model.named_steps['normalizer'].predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}